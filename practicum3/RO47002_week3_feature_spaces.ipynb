{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RO47002 Machine Learning for Robotics\n",
    "* (c) TU Delft, 2024\n",
    "* Period: 2024-2025, Q1\n",
    "* Course homepage: https://brightspace.tudelft.nl/d2l/home/682421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`. Moreover, if you see an empty cell, please DO NOT delete it, instead run that cell as you would run all other cells. Please fill in your name(s) and other required details below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_NUMBER = \"23\"\n",
    "STUDENT_NAME1 = \"Daniel Rugge\"\n",
    "STUDENT_NUMBER1 = \"4713729\"\n",
    "STUDENT_NAME2 = \"Daan Bouwmeester\"\n",
    "STUDENT_NUMBER2 = \"5146143\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3f76d6a626db81c484191482b101edb",
     "grade": true,
     "grade_id": "cell-c35e4c8223095209",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Note: this block is a check that you have filled in the above information.\n",
    "# It will throw an AssertionError until all fields are filled\n",
    "assert(GROUP_NUMBER != \"\")\n",
    "assert(STUDENT_NAME1 != \"\")\n",
    "assert(STUDENT_NUMBER1 != \"\")\n",
    "assert(STUDENT_NAME2 != \"\")\n",
    "assert(STUDENT_NUMBER2 != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General announcements\n",
    "\n",
    "* Do *not* share your solutions, and do *not* copy solutions from others. By submitting your solutions, you claim that you and your lab partner alone are responsible for this code.\n",
    "\n",
    "* Do *not* email questions directly, since we want to provide everybody with the same information and avoid repeating the same answers. Instead, please post your questions regarding this assignment in the correct support forum on Brightspace, this way everybody can benefit from the response. If you do have a particular question that you want to ask directly, please use the scheduled practicum hours to ask a TA.\n",
    "\n",
    "* There is a strict deadline for each assignment. Students are responsible to ensure that they have uploaded their work in time. So, please double check that your upload succeeded to the Brightspace and avoid any late penalties.\n",
    "\n",
    "* This [Jupyter notebook](https://jupyter.org/) uses `nbgrader` to help us with automated tests. `nbgrader` will make various cells in this notebook \"uneditable\" or \"unremovable\" and gives them a special id in the cell metadata. This way, when we run our checks, the system will check the existence of the cell ids and verify the number of points and which checks must be run. While there are ways that you can edit the metadata and work around the restrictions to delete or modify these special cells, you should not do that since then our nbgrader backend will not be able to parse your notebook and give you points for the assignment. You are free to add additional cells, but if you find a cell that you cannot modify or remove, please know that this is on purpose.\n",
    "\n",
    "* This notebook will have in various places a line that throws a `NotImplementedError` exception. These are locations where the assignment requires you to adapt the code! These lines are just there as a reminder for youthat you have not yet adapted that particular piece of code, especially when you execute all the cells. Once your solution code replaced these lines, it should accordingly *not* throw any exceptions anymore.\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "faaef8f6085919e6a9f4bc40c8ee180c",
     "grade": false,
     "grade_id": "cell-ea8199147d493fd9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Practicum 3\n",
    "* Topic: SGD, classification metrics, Bayesian classifier, overfitting\n",
    "* Before performing this practicum, work through **Book chapter(s): 3, 4**\n",
    "* **Deadline**: Monday, September 23, 2024, 23:59\n",
    "\n",
    "## Objectives\n",
    "\n",
    "* Explore randomness in SGD optimization\n",
    "* Reimplement binary classification metrics\n",
    "* Decision boundaries in the feature space\n",
    "* Reimplement linear classification hypothesis function\n",
    "* Implement a Bayesian classifier\n",
    "* Implement a Gaussian-Mixture Bayesian classifier\n",
    "* Visually understanding model complexity, overfitting, Bias & Variance\n",
    "* Explore the effect of using regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup common python stuff\n",
    "We will start by loading a few common python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python â‰¥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn â‰¥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import skimage\n",
    "import skimage.transform\n",
    "import skimage.util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up a small dataset\n",
    "\n",
    "For the next few sections, we will use one of sklearn's builtin datasets to provide some classification data to toy with. We'll only use 2 features, so we can later easily plot and inspect this 2D feature space, and see how it relates to classifier performance. We'll also turn it onto a 2 class problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "789b990eb07c70dadf14fa06260d153d",
     "grade": false,
     "grade_id": "cell-f4ba51bd1c8bf36b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Take a standard dataset that comes with sklearn\n",
    "ds = datasets.load_wine()\n",
    "X = ds['data']\n",
    "X = X[:,:2]\n",
    "y = ds['target']\n",
    "y = (y == 2).astype(np.int32) # make this a binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Test split')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAroAAAELCAYAAAA2gyVUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAABP/ElEQVR4nO3de5hcVZnv8e9b1Z0LJEAnaLgEEmKAQAKBdELCMJE2ZkbhwXFEOSMXIecMMscZUMcBb6BGQBwRPTNzxMEADhcDysVRThxEEVqiEhMaRdMQhiSkk+YmJh1Im0t3V63zx67q7K6ue9Wu2rvq93mehlTVrqq1q2rt9e613rW2OecQEREREWk0sXoXQEREREQkCAp0RURERKQhKdAVERERkYakQFdEREREGpICXRERERFpSAp0RURERKQhKdCNCDO7w8yuT/27w8x6S3hup5ldGlzpRCQfM3vYzC6p8XtuMbOlqX9/1sxuq+X7i0h+ZjbdzJyZtaRu1/w40QwU6IZMKijtM7OxdXr/4cZRpJmZWb/vL2lme3y3LyzltZxzZznn7gyqrEW8/w3OuUthdOMq0miqWXdTr1eTziL/ccLMlpnZL4J+z2agA12ImNl0YDHwBvBXwP11LZBIE3POTUj/28y2AJc65x7N3M7MWpxzQ7Usm4jkVmzdleagHt1wuRhYA9wBlD18YWZ/YWYbzOwNM/sGYL7H3mZmj5nZdjP7o5mtNLNDUo/dDRwN/L/Ume8nU/ffb2avpl7vCTObXf4uikRbOnXIzD5lZq8C/2FmbWa2ysxeT43IrDKzqb7nDPcIpXtqzOym1LYvmtlZed7vU2b2kpntMrPnzeydqfuXm9kDZva91GNPm9ncHK+x3My+k7r5ROr/O1P1/PSqfDAiIWdmMTP7tJltSrWB95nZpNRj48zsO6n7d5rZOjObYmZfwuuA+kaqvnwjy+tmfW7qsU4z+7KZrTWzN83sh+n3zPI6nWZ2qZmdANwCnJ56z52BfShNQIFuuFwMrEz9vStdUUphZocC3weuAQ4FNgFn+DcBvgwcAZwAHAUsB3DOfQjYCrzHOTfBOXdj6jkPA8cCbwWeTpVPpJkdBkwCpgGX4R1L/yN1+2hgDzCqQfRZCDyPV0dvBG43M8vcyMyOBy4HFjjnJgLvArb4Nnkv3sjPJOAe4Adm1lqg7G9P/f+QVD1/ssD2Io3iCuCvgTPx2sA+4ObUY5cAB+O1iZOB/w3scc5dDawGLk/Vl8uzvG7W5/oevxj4X8DhwBDwb/kK6Zx7LvUaT6be85BSd1T2U6AbEmb253iN5H3OuS68APWCMl7qbKDbOfeAc24Q+Bfg1fSDzrmNzrmfOuf2OedeB76OV+lzcs592zm3yzm3Dy8onmtmB5dRNpFGkQS+kKpHe5xz251zDzrndjvndgFfIn+96nHO3eqcSwB34jWA2U5sE8BY4EQza3XObXHObfI93uWr618HxgGLqrGDIg3ofwNXO+d6fe3ZB1L56oN4QepM51zCOdflnHuzyNct9Ny7nXPrnXN/Aj4H/A8zi1dtryQvBbrhcQnwE+fcH1O376G89IUjgG3pG84557+dGor5bmoo9E3gO3i9SlmZWdzM/jk11PMm+3uTcj5HpAm87pzbm75hZgeY2bfMrCdVT54ADsnTmPlPPnen/jkhcyPn3Ebg43gN8h9SdfcI3yb+up4EevGOASIy2jTgP1PpBTuB5/BOJqcAdwOPAN81s5fN7MYiRkfSCj13m+/fPUArakNrRoFuCJjZeOB/AGemcmFfBf4Rr+c0a85dHq/gDZ+kX9v8t4EbAAec5Jw7CLgIXw5v6jG/C/CGR5fiDc1MT790ieUSaSSZ9eSfgOOBhal6lU4PqLieOOfucc6lR3wc8BXfw/66HgOmAi8XeslKyyQSUduAs5xzh/j+xjnnXnLODTrnvuicOxH4M+AcvJQDKFBnCjwXRrbBR+P1AP+R/FRPq0SBbjj8Nd5Z5YnAKam/E/Dygi7O9aQcfgTMNrNzU8MxH8XLJ0ybCPQDb5jZkcBVGc9/DZiRsf0+YDtwAF6gLCIjTcTLyduZmmjyhWq8qJkdb2ZLzFtucG/qPZK+Tdp9df3jeHV1TYGXfT31GjMKbCfSaG4BvmRm0wDM7C1m9t7Uv99hZielRmHexAtG03Uts10cocBzAS4ysxPN7ADgWuCBVNpSPq8BU81sTOm7KX4KdMPhEuA/nHNbnXOvpv/wJrNcaCWsd5lKfTgP+Ge84PRY4Je+Tb4IzMNbwuxHeBPX/L4MXJMa2rkSuAtvqOUl4FkKN6IizehfgPF4vTRrgB9X6XXH4tXlP+KlO7wV+Izv8R8Cf4M3qeZDwLmpfN2cUqkSXwJ+marnyumVZvGvwEPAT8xsF15dXZh67DDgAbxA9Tng53gpCennfSC1Skq2iWT5nkvq33fg1eFxeB1QhTwGdAOvmlmh3l/Jw7wUThERiRIzW443+eWiepdFRLIzs07gO845XZmwTtSjKyIiIiINSYGuiIiIiDQkpS6IiIiISEMqukfXzD5oZs+Z2Z9Sa6ouDrJgIlI+1VcREREoaja/mf0F3tqNfwOsxbuKT16HHnqomz59ekWFi6I//elPHHjggfUuRuQ0w+fW1dX1R+fcW4J+n7DU17B/pypf5cJexkrKV6v6Wo7M+hr276EU2pfwCvP+5K2vzrmCf8CvgL8tZtv0X3t7u2tGjz/+eL2LEEnN8LkBT7kS6lC5f2Gpr2H/TlW+yoW9jJWUr1b1tZy/zPoa9u+hFNqX8Arz/uSrrwV7dFMLIM8HHjKzjXhrwP0AuMo5tydj28uAywCmTJlCZ2dnOYF5pPX39zflfldKn1t1hKm+hv07VfkqF/Yyhr18IhK8YlIXpuBdl/kDwGK8K378ELgGuNq/oXNuBbACYP78+a6jo6OaZY2Ezs5OmnG/K6XPrWpCU1/D/p2qfJULexnDXj4RCV4xk9HSvUD/1zn3ivOuvPV14OzgiiUiZVJ9FRERSSnYo+uc6zOzXsC/DllZa5INDg7S29vL3r17y3l6JBx88ME899xzRW8/btw4pk6dSmtra4ClkmYRpvpaal2otXLLpzorYdQo7Wu1jxuqr1LUqgvAfwBXmNmP8YZC/xFYVeqb9fb2MnHiRKZPn46Zlfr0SNi1axcTJ04salvnHNu3b6e3t5djjjkm4JJJEwlFfS2lLtRDOeVTnZWwapT2tZrHDdVXgeLX0b0OWAf8N/Ac8BvgS6W+2d69e5k8eXKkK2E1mRmTJ0+O/Bm4hI7qa0BUZyWsVF9HU30VKLJH1zk3CPx96q8iqoQj1frz6OrpY83m7SyaMZn2aW01fW+pDdXXYNXyM1F9lVKovo7WUJ/JtrWwZTVMXwxHnVbv0kRGsakL0gC6evq48LY1DAwlGdMSY+Wli9R4ioTUxr4EN/1M9VVE8ILcO/8KEgMQHwOXPKRgt0hFXwK4kS1fvpybbropkNfu6uripJNOYubMmXz0ox9NL+hfF2s2b2dgKEnSweBQkjWbt9etLCLlapb6umFHQvVVIq9Z6mvgtqz2glyX8P6/ZXW9SxQZCnQD9pGPfIRbb72VF154gRdeeIEf//jHdSvLohmTGdMSI27Q2hJj0YzJdSuLSBiFqb7OmhRXfRXJI0z1NXDTF3s9uRb3/j99cb1LFBmhD3S7evq4+fGNdPX0VeX17rrrLk4++WTmzp3Lhz70oVGP33rrrSxYsIC5c+fy/ve/n927dwNw//33M2fOHObOncvb3/52ALq7uznttNM45ZRTOPnkk3nhhRdGvNYrr7zCm2++yaJFizAzLr74Yn7wgx9UZT/K0T6tjZWXLuITf3m8hkElEFGqrxs3bhzxWmGrrzPb4qqvEWdmx5rZXjP7To7Hl5vZoJn1+/5m1Kp8Yayv7373u4Hota+BO+o0L11hydVKWyhRqHN0q51T2t3dzfXXX8+vfvUrDj30UHbs2DFqm3PPPZcPf/jDAFxzzTXcfvvtXHHFFVx77bU88sgjHHnkkezcuROAW265hY997GNceOGFDAwMkEgkGBoaGn6tl156ialTpw7fnjp1Ki+99FLZ5a+G9mltajAlEJn19dYLTmbxCeUvExR0fU3fn6b6KgG4GW8FlHy+55y7qBaF8Qtr+7pt2zYge/vqF8b6GrijTlOAW4ZQ9+hWO6f0scce47zzzuPQQw8FYNKkSaO2Wb9+PYsXL+akk05i5cqVdHd3A3DGGWewbNkybr311uEKd/rpp3PDDTfwla98hZ6eHsaPH19R+RrKtrWw+mve/6UpZNbXp3p2VvR6qq81pjpbVWb2QWAn8LM6FyUrta/SLELdo5vOKR0cStYsR23ZsmX84Ac/YO7cudxxxx10dnYC3tnlr3/9a370ox/R3t5OV1cXF1xwAQsXLuRHP/oRZ599Nt/61rdYsGDB8GsdeeSR9Pb2Dt/u7e3lyCOPDHwf6k6zQ5tSZn2dP+2QwN+zkvr6f/7P/+Gcc84Zfq2mra+gOltlZnYQcC2wBLi0wObvMbMdwCvAN5xz/57jNS8DLgOYMmXK8G8doL+/n4MPPphdu3YVXcaTDxvPmHiMwUSS1niMkw8bX9LzM+3du5eBgYFRr7Fv3z5aW1vZtWsXl1xyCffcc89woLt69Wp27drFV7/6VdatW8cjjzzCddddxxNPPMF73vMeZs+ezSOPPMK73/1u/vVf/5Uzzzxz+HUPPvhgtm7dOvx+L7zwAm9961uz7sPevXtHfF610t/fX5f3DUpU9yfUgW46p7Ra60guWbKE973vfXziE59g8uTJ7NixY9RZ565duzj88MMZHBxk5cqVww3dpk2bWLhwIQsXLuThhx9m27ZtvPHGG8yYMYOPfvSjbN26ld/97ncjAt3DDz+cgw46iDVr1rBw4ULuuusurrjiior2IRKyzQ5Vo9nwMuvrcZMqO7wEXV/Xr18/ItBt2voKqrPVdx1wu3Out8A6rvcBK4DXgIXAg2a20zl3b+aGzrkVqW2ZP3++6+joGH6ss7OTcePGlXRFscUnTGTlhw+oWvt69tln8773vY9Pf/rTI+rr2LFjGTt2LBMnTqS/v5+ZM2cybtw4HnzwQY488kgmTpzIpk2bWLJkCUuWLOGnP/0pO3fuJJlMDuf7/uEPf2Djxo0j6uvEiRM55JBD6O7uZuHChdx///1cccUVWT+DcePGceqpp1a0f+Xo7OzE/z1FXVT3J9SBLlQ3R2327NlcffXVnHnmmcTjcU499VTuuOOOEdtcd911LFy4kLe85S0sXLhw+Ozwqquu4oUXXsA5xzvf+U7mzp3LV77yFe6++25aW1s57LDD+OxnPzvqPb/5zW+ybNky9uzZw1lnncVZZ51VlX0JtfTs0HTvkGaHNg1/fa2kdwiCr6/f+ta3Rr1nU9ZXUJ2tIjM7BVgKFIysnHPP+m7+ysz+FfgAMCrQDUIY29fFixerfZXqcs4F8tfe3u4yPfvss6PuazRvvvlmyc9pyM9l66+de+Im7/9FePzxx4MtTwgAT7mA6lulf0HU13LqQi1VUr5a1Nma14kS66xz4a+3lZSv3PoKfBz4E/Bq6q8f2AM8XcRzPwV8v9B2mfX18ccfb5h2JIjjRr0+m7DXj1KFeX/y1dfQ9+hKRGl2qEi0qM5Wywrgu77bVwLTgY9kbmhm7wWewJu0tgD4KDC661JEyhbqVRcEzYQWiRLV16bnnNvtnHs1/YfXo7vXOfe6mS02s37f5h8ENgK7gLuArzjn7qxDsUUalnp0w0wzoUWiQ/VVsnDOLff9ezUwwXf7/HqUSaSZqEc3zHRta5HoUH0VEQkdBbphpmtbi0SH6quISOgodSHM0te23rLaazQ1DCoSXqqvIiKhox5dYPny5dx0002BvPbVV1/NUUcdxYQJEwpvnM1Rp8Hif1KjKZKi+ioSHaGur9IUFOgG7D3veQ9r12oGtkgUqL6KRIfqqxQj/IFulZfrueuuu4YvK/ihD31o1OO33norCxYsYO7cubz//e9n9+7dANx///3MmTOHuXPn8va3vx2A7u5uTjvtNE455RROPvlkXnjhhVGvt2jRIg4//PCqlF0k9CJUXzdu3Djq9VRfpamEsL6++93vBtS+SvWEO0e3ysv1dHd3c/311/OrX/2KQw89lB07doza5txzz+XDH/4wANdccw233347V1xxBddeey2PPPIIRx55JDt37gTglltu4WMf+xgXXnghAwMDJBIJhoaGyi6fSKRl1NfYed+F4zvKfrmg62v6fpGmFNL2ddu2bUD29lWkHOHu0a3ycj2PPfYY5513HoceeigAkyZNGrXN+vXrWbx4MSeddBIrV66ku7sbgDPOOINly5Zx6623Dle4008/nRtuuIGvfOUr9PT0MH78+IrKJxJpGfW1ZduTFb2c6qtIgNS+SpMId6Bbh+V6li1bxje+8Q1+//vf84UvfIG9e/cC3tnl9ddfz7Zt22hvb2f79u1ccMEFPPTQQ4wfP56zzz6bxx57LPDyiYRWRn0dOur0wN+ykvr685//PPDyiYRWSNvXM888U+2rVFW4A930cj1Lrq7KVYaWLFnC/fffz/bt2wGyDq3s2rWLww8/nMHBQVauXDl8/6ZNm1i4cCHXXnstb3nLW9i2bRubN29mxowZfPSjH+W9730vv/vd7yoqn0ikZdTX5BHtFb1c0PV1/fr1FZVPJNJC2r5OnjxZ7atUVbgDXajqcj2zZ8/m6quv5swzz2Tu3Ll84hOfGLXNddddx8KFCznjjDOYNWvW8P1XXXUVJ510EnPmzOHP/uzPmDt3Lvfddx9z5szhlFNOYf369Vx88cWjXu+Tn/wkU6dOZffu3UydOpXly5dXvB9VV+UJCdLEIlRfzz9/9NVXI1FfRaolhPV14cKFjdW+St2Zcy6QF54/f7576qmnRtz33HPPccIJJwTyfmGxa9cuJk6cWNJz6vq5VHlCQrk6Ozvp6Oio+fvWkpl1Oefm17sc2QRRX8upC7VUSflqUWdz1olta0NzUYqw19tKyhel+trZ2cmUKVMaon0N4rhRrzY27PWjVGHen3z1NdyrLkjwsk1I0GL3IuEUkhNTEZGoCH/qggSrDhMSRKRMVZ4pLyLS6Greo+ucw8xq/bahFVTqSNHSExJCMhQq4aL6Olpd62z6xDTdo6sTU/FRfR2t7m2s1F1NA91x48axfft2Jk+erMqIVwG3b9/OuHHj6luQo05TgCujqL6OVvc6qxNTyUH1dbS619cm1NXTx5rN21k0YzLt09rqXRygyEDXzDqBRUD6sl8vOeeOL/XNpk6dSm9vL6+//nqpT42MvXv3llSpxo0bx9SpUwMsUciEaCKN5FdpfS21LtRaueWre53ViWlkmNmxwO+BB5xzF2V53IB/Bi5N3XUb8GlXRjdko7Sv1T5u1L2+NpGunj4uvG0NA0NJxrTEWHnpolAEu6X06F7unLutkjdrbW3lmGOOqeQlQq+zs5NTTz213sUIJ02kqZlqnJxWWl/DXhfCXr5Q0QlquW4G1uV5/DLgr4G5gAN+CrwI3FLqGzVK+6p6GV1rNm9nYChJ0sHgUJI1m7eHItDVZDSpHU2kqbXLnXMTUn8lj8CIAPtPUB/7kvd/rbddFDP7ILAT+FmezS4Bvuac63XOvQR8DVgWfOlEqm/RjMmMaYkRN2htibFoxuR6FwkorUf3y2b2z8DzwNXOuc7MDczsMrwzVKZMmUJn56hNGl5/f39T7vdBb2zgkJ3r2XnIHN48eFaObQ5krsUx53AW55kdB/Jm6rNq1s9NJPS0BGHJzOwg4FpgCfvTErKZDTzju/1M6r5sr5mzfW2k46f2JbyK2Z8r541hw44EsybF2fXiM3S+WJuy5VPUBSPMbCHwLDAAfBD4BnCKc25TrudkW4C+GYR5QeXAlJKSkGMItBk+t1ouQJ9KXZgNGDlOTjMazvbvfve7VS1Df38/EyZMqOprVlOzl6+Yk9P0dnOf+RyWHMLFWnhm7nXD2zfyZ/iOd7yj7PpqZv8KvOyc+4qZLQdm5sjRTQCznXMbUrePBf4biOXL0812wYhGOX5qX8IrzPtT8QUjnHO/9t2808zOB84G/m8VyidRV0qPjybS1MqnGHly+v/MbMTJqXNuBbACvIaz2gewMB8UocnLt20t3Lm8yHz5Dpg3b/gEdV6ETlDrUT4zOwVYChSTaNoPHOS7fRDQX85kNBHJrtzlxRxeT5GI1vYMIZ2cSl6lpiPoBLUUHcB0YGtqma8JQNzMTnTOzcvYthtvIlo68Xlu6j6R0TQptCwFA10zOwRYCPwcbwb33wBvBz4WaMkkOrS2ZxTo5FT208lpkFYA/jygK/EC349k2fYu4BNm9l94dfSf0MmoZKNVi8pWTI9uK3A9MAtIABuAv3bO/XeQBZOIUY9PaOjkVArSyWlgnHO7gd3p22bWD+x1zr1uZouBh51z6cThbwEz8NbaBW8d3W/VsrwSEZoUWraCga5z7nVgQQ3KIiLVoZNTKUwnpzXhnFvu+/dqvFSG9G0HfDL1J5Lb9MUkY63eET3WSkyjMEWr6SWARSR4OjkVEWksXclj+erAZ2l33XQlZnNV8lja612oiFCgKyIiIhJiazZvZ+3QTNa4mcSN0Fx1LAp0ZbR62LYWVn9NVxgSERGRgsJ61bEoUI9urWnmpIiIiJSgfVobKy9dxJrN21k0Y7J6c0ugQLfWNHNSJDq0bqWIhET7tDYFuGVQoFtrWr9SJBo0+iIiEnkKdGtN61eKRINGX0REIk+Bbj0EvX5lEMOtGsKVZqPRFxGRyFOg22iCGG7VEK40o1qNvugkUkQkMAp0G00Qw60awpVmVYvRF51EiogERuvoNpr0cKvFqzfcGsRrikj2k0gREaka9eg2miCGWzWBTiQYygMWEQmUAt1GFMRwa9BDuCLNSCeRIiKBUqArIlJPOokUaU6aiFoTCnRDpKunT5f3ExERaXSaiFozmowWEl09fVx42xq+9pPnufC2NXT19NW7SCKSR1dPHzc/vlF1VUYxs++Y2Stm9qaZ/beZXZpju2VmljCzft9fR21LK3Whiag1ox7dkFizeTsDQ0mSDgaHkqzZvD36vboalpEGlT4xHRhKMqYlxspLF0W/vko1fRn4W+fcPjObBXSa2W+cc11Ztn3SOffnNS6f1JsmotaMAt2QWDRjMmNaYgwOJWltibFoxuR6F6kyGpaRBtaQJ6ZSNc65bv/N1N/bgGyBrjQjTUStGQW6Zap2Pm37tDZWXrqocXJ0dZEJaWANd2KaplGYqjGzbwLLgPHAb4D/yrHpqWb2R2AHcDfwZefcUE0KKfWliag1oUC3DIWGLcsNgtuntUU/wE3TsIw0sIY7MQWNwlSZc+7vzewK4HSgA9iXZbMngDlADzAb+B4whJf6MIKZXQZcBjBlyhQ6OzuHH+vv7x9xO8q0L+EV1f1RoFuGfMOWG/sS3PSziOfuVaNXR8MyEhJBrWbSUCemoFGYADjnEsAvzOwi4CPAv2U8vtl38/dmdi1wFVkCXefcCmAFwPz5811HR8fwY52dnfhvR1VXTx+rHl3H+SfNDWXdKvVY0ijfS1pU90eBbhnyDVtu2JGIdu5eNXt1NCwjdRbU6EuoVCvdQKMwQWrBy9EtxAEWcFlCKV1X9w0mWbVlTeg6iTQBNboU6JYh37DlrElxxrQkopu7p14daSBNMfpSzRNTjcJUzMzeCiwBVgF7gKXA+am/zG3PAp52zr2WWp3hc8D9NSxuaKTrqiOcnUSagBpdCnTLlGvYcmZbPNq5e+rVkQbS0KMvUP0TU43CVIPDS1O4BW+t+h7g4865h8zsaOBZ4ETn3FbgncAdZjYBeA34DnBDfYpdX+m6OjAYzk6ihp2A2gQU6AYg0rl76tWRBtLQoy+gE9MQcs69DpyZ47GtwATf7SuBK2tUtFBL19V7H13H+UsXhK4NbcgJqE1Cga6Mpl4daSANO/oCOjGVhtI+rY1dbxsT2roY6U6sJqZAV2pHa3RKyDREw6UTUxGRnBToSm2UOmlGQbGIiIhUSIGu1EYpk2a0cL1IOOiEU0QirqRA18yOBX4PPOCcuyiYIklDKmXSTEZQ/NJvf8IPNk6Kbh6lSBSVc8KpwFhEQqbUHt2bgXVBFEQaXCmTZnxBcTLWyj+tncjaoeeju9apSBSVunSZRmJEJISKDnTN7IPATuBXwMygCiT1E/hVooqdNOMLir+//RjW/nosSQf7BpN8/+leBboitVDq0mW62IyIhFBRga6ZHQRci3e1l0sDLZHURegub5gKio/p6aPlqTXDV8y5/6ltnDtvqoLdIijVSCpS6tJlWtNXREKo2B7d64DbnXO9Zrkvw21mlwGXAUyZMoXOzs6KCxg1/f39kdzvVZsG2DfoBZMDg0nufXQdu942pmbvn+9zO+Mw4/Fe799DCVfzskWYUo2kMqUsXZYlMA58lEhEpICCga6ZnYJ3re5TC23rnFsBrACYP3++6+joqLB40dPZ2Uno9zvLhJGJx/Sxasua4atEjboyTcCTTPJ9bhOP6ePJ2/KUTUZRqlFzCF0g6QuMQzdKJCJNqZge3Q5gOrA11Zs7AYib2YnOuXnBFa2B1XNmco4JI3kvb1jnSSa69GJpik01CnoEJuyjG1Ev38a+BDeu28tgElpj8MkF45jZFq9dAclfxnqPEhUqn4g0h2IC3RXAd323r8QLfD8SRIEaXpWCxnJ7cl767U84fGgfMZKjJozkvEpUCCaZNMQVrGqnqFSjoEdgwj66EfXydT++kSH3PA5IONh3yDQ6OmrbeV9oJCbvKFGdyyfSELSkX0EFA13n3G5gd/q2mfUDe51zrwdZsIaVLWhM31/kD7XcIcGunj6+unYi/xFroZUhYvFWYsVMGJm+mGSsFRJArMjnSF2UkmokEZKlMVs0YzJjWmLDgeSiGZPrXMiRNBIjErACHWehS22qk5KvjOacWx5AOZpH5szk8ZNL7uFds3k7A0NJkg4Gh5Ks2bw9648480e+ZvN21g7N5EI+y+nx5zjm1HfzgWIC6+SxfHXgs7S7broSs7kqeSztZX8AErAOlGpUPVXqLSm3wenq6ePF3zzOub//CLHkYPHpRiGhkRiRAOUZbVWO/H66BHCtZc5MLiMtoJienGw/8vTznhk6jmdtFitPXVRUkdMB8ho3k7iRM7CWUFCqUbVUMc2onAZnY1+Cm362hr91PyYZHyBmJaQbiUjjy7OkX7EdYs1AgW49ZC7ZU+Lak8X05GT7kf/DO2aW1QMU9iFS2U+pRlWU6yS0xF7echucDTsSDAwleZITuDzeQowEMa1PKyJpeda6Vru9X9MGuqHJXSl1UfaUQj05uX7k5fQARWGIVLJTqlEFsvWWlNHLW0qD4z8uzZoUZ0xLgmeGjuN/Jq/ha6ft4shT/lITTiLAzL4DvBM4EHgVuNE5d1uObf8R+BRwAPAA8BHn3L5alVUiLsda12q392vKQDd0uSulLMpepGr/yIcD5G1rYbVmeEoTyHYSuvprJacaFVsXM49LV84b43ven3FkEzdUEfRl4G+dc/vMbBbQaWa/cc51+Tcys3cBn8ZbCvBl4D+BL6buE6mIUps8TRnoNkvuStV/5HVeT1eaT91HXjJPQsu8zG0xdTHzuLRhR4JL1VBFknOu238z9fc2oCtj00vwlgLsBjCz64CVKNAVqZqmDHSVu1KmEKynK80jdCMvUHaqUTEyj0uzJtX24g9SXWb2TWAZMB74DfBfWTabDfzQd/sZYIqZTXbObc94vZwXeGmkC2NoX8IrqvvTlIGuclfKVGZvlkg5QjvyEkCqEYw+Lu168ZnqvLAWlK8L59zfm9kVwOl4y/5ly7udALzhu53+90RgRKCb7wIvjXRhDO1LeEV1f5oy0AXlrpQlwN4skUzNOPLiPy51vliFF1S6UV055xLAL8zsIrwl/v4tY5N+4CDf7fS/d9WgeCJNoWkDXUkptbcnvU36im51bjTrnsMpgdHISxUo3SgsWvBydDN1A3OB+1K35wKvZaYtiEj5FOhGVTWGI8vp7QlRD1EoczilqjTyUiGlG9Wcmb0VbxWFVcAevEtyn5/6y3QXcIeZrcRbdeEa4I7alFSkOcTqXQApQzrYfOxL3v+3rS3vdbL19gTxnIBky+EUEZ90ulH7JXDKB+tdmmFdPX3c/PhGunr66l2UIDi8NIVeoA+4Cfi4c+4hMzvazPrN7GgA59yPgRuBx4GtQA/whfoUW6QxqUc3iqo1HFlOb0+IeoiaMYdTpCy/vders7/9bt3zdBt9JCZ1FcIzczy2FW8Cmv++rwNfr0HRRJqSAt0oqlawWc7kshBNSFMOp0RGPVc+CFmebmhX0xCRhqRAN4pKCTYLNbDlLJUU0PJK5VAOp4RevfPaQzQKAxqJkQampfxCSYFuFXX19LFq0wATj+kLPvjyBZs5Vx6odwMrIvXvUQ3RKAxoJEYaVLb2VkJBgW6VpPPO9g0mWbVlTc3yzvLmu9W7gRWRcPSohmgUBjQSIw0o60Tt9nqXKrcm6n1WoFsl6bwzR23zzvz5bnMSGxjoXA1L/tr74YahgRVpdqX2qKYaoIPeOBDvgloiEnrZ2ttNu+tdquyabLRXgW6VpPPOBgZrm3eWft85iQ3c3XoD47YMwZ3f3v/DDdGQpUjTKrZH1dcAzbU4zJuneisSBdna202d9S5Vdk022qtAt0rSeWf3PrqO85cuqNmwXPp9BzpXM27LEOaSI3+4IRuyFJE8fA2QOdfwDZBIQ4lKe9tko70KdKuofVobu942pua5Z+3T2rx0hTu/XdkPt4lydkTqLlt98zVAzuIN3wCJSB002WivAt1Gkf7hPnMv3oV5SlTNnB0FzCL55apvvgbomR0HMk/1R0SCEJXe5ypQoNtoyr0CUq5L+5YasDZZkrtIWfLlyKUaoDc7O+taRBGRRhCrdwGkinIFq8VID5la3Pv/+MlewPrYl7z/b1sbfBlEmkVmfSs3RWHbWlj9teLrp4hIk1GPbiOpJME8M2en3FmZTZbkLlKW4VSjewAr7zU0eiIiUpAC3TrJeTWzSlSaYJ6Zs5MvYM2Vh9tkSe4iFfntd1OpRveWHqhmnow+c0/59U559SLSoBTo1kHeq5lVqloJ5vkC1kI9SU2U5C5StkrXsvSPnsTi8Jt7IDlUeu+ueoZFpIEpR7cO/FczS19FLZSOOg0W/9PoRm/Lahja5zXQQ/uUhytSjkrzdNMno0uuhlMv8oLccnLjlVcvEm7Kxa+IenTrIH01s8Gh2l5FrWrGTwaSqRvJ1O2A0jFEQiCUqUbp1zjqNK8BTK+4kitozpWeoLx6kfDSiEvFFOjWQfpqZpENCvdsB4uBS3r/37M92HQMkTqKfKoR5G8slVdfVWY2FvgmsBSYBGwCPuOcezjLtsuA24E9vrvPcc51Bl9SiYQmu1xvEIoKdM3sO8A7gQOBV4EbnXO3BVmwhuTrUWmfdlruxjLsE0OmL4b42BE9QGs2jk7HUKArjSBbqlEof9v5guZCjaXy6qupBdgGnAlsBc4G7jOzk5xzW7Js/6Rz7s9rWD6JEo24VKzYHt0vA3/rnNtnZrOATjP7jXOuK8CyNZZihx+iMEyRpQdoUbIv2ukYIjlEPtUI1FjWkHPuT8By312rzOxFoB3YUo8ySYRpxKViRQW6zrlu/83U39sABbrFKnb4oRrDFLXoEc7oAYp8OoZIDg3x21ZjWTdmNgU4DujOscmpZvZHYAdwN/Bl59xQlte5DLgMYMqUKXT6rpzX398/4naUaV9yaYdNu2FTtV6vdFH9borO0TWzbwLLgPHAb4D/CqhMjanYHpVKe16eugP+658gmYSWsTXtEW6f1hbNIECkgIb4bSs9oebMrBVYCdzpnNuQZZMngDlADzAb+B4whDeKOoJzbgWwAmD+/Pmuo6Nj+LHOzk78t6OsVvtSi8nTjfS9QHT3p+hA1zn392Z2BXA60AHsy9wm3xlns8h3xnPQScs5ZOd6dh4yhzfznJkVu92o572xgVN++1nMJTAgObSXLY/dxdZpu8val1qK6pliGCmnvgqKHRUJez691I2ZxfB6aAeAy7Nt45zb7Lv5ezO7FriKLIGuVI8mTzeXklZdcM4lgF+Y2UXAR4B/y3g85xlns8h/xpPr/nK3y7C6Cy+rxBOLxZmx5GJmRKABjuqZYkgpp74S9cinV8DcUMzM8FZTmAKc7ZwbLPKpjrKvCS3FKnuCqeppJJW7vFgLXo5uUwrterHDqyHs85b9OvtrqoxNSDn1FaplPj1EYwKqlOrfgROApc65Pbk2MrOzgKedc6+lTko/B9xfozI2rbImmKqeRlbBQNfM3gosAVbhrfW3FDg/9dd0Qj3koQknklJMTn3QqUZhT0fJVb6D3jiQuRbHnMNZnGd2HMibFWyXz0FvbGD6lntpG9qL4UgO7RtONwr75wfR/Y6DZGbTgL/DS+971evchdR9q4FngROdc1vxUozuMLMJwGvAd4AbalrgJlTWBFOtZxtZxfToOrw0hVvwLhncA3zcOfdQkAULq2qtqRlYr7AmnAjF5dQHnWoU9nSU3OXrgHnzhk8Y5+WsT8Vul8O2tXDncu8y2jiwGLH42OF0o7B/fhDl7zg4zrke8qcfTPBteyVwZeCFklFKnmCqJfoiq2Cg65x7HW/ha6E6a2rWrFdY+URNrVBOfTMo+4Sy2BPGSk4s0z1EJIEYzOiAjs+oroqEkUZMI0uXAC5RNdbUrMmVlpRPJPs1ZU59qNOMYHQPkYJckXDTiGkkKdAtQ6VratbkSkthyidSz3LNKKd+v9Bfulc9RCIigVOgWwc1udJSWPKJ1LNca8qpT6nmCaVy6kVEokmBbp0EfqWlsPQWhalnuQkop36/ap1Qhj4FQkREclKgW6TQrp2bTxh6i8LSsyxNqRonlKFPgRARkZwU6BYhEj06Yc2DDUvPskiZapJTLyIigVCgW4TQ9+iEPQ82DD3LImWqSU69iIgEQoFuEULfo6M8WJFABZ5TLyIigVCgW4TQ9+hUmgebSnvYMG4uP+ufXvo+hjVtQppSJPLpVWdERGpCgW6RatGjU9FVnMrNg02lPbjEPqYlW3hs8LP83/is/XnIhRrksKdNSFOJTD696oyISE0o0A2JihvocvNgU2kP5pK0MsRCe47fDh3n5SHHXijcICttQkIk9Pn0oDojIlJDsXoXQDzZGuiaSKU9OIszSAtr3Qn785CzNcg5no/Fq7982La1sPpr3v9FipDOp48b4cynh8rrjK9edPX0cfPjG+nq6QumrCIiEace3ZCo24S3VNqDbVlNz7i5vKN/Op9Jp07Eisj9DWr5MA3vShlCn08PVUk1IjFAMtbKVwc+y9qhmaWPAilHWESahALdOvPn5eZtoINsmFJpD7OAWZn3F9MgB7F8mIZ3pUy1WiGhoklvFaYaefUC2l03a9zM0tI0dBIpIk1EgW4dZcvL/Yd3zBzx+JrN23nnhC3MeuSi+jRM9VoDV1dUkxCr26Q3f72ItdKVmF16moZOIgNlZmOBbwJLgUnAJuAzzrmHc2z/j8CngAOAB4CPOOf21ai4Ig2vKQPdsCw/lG/ijL8h3df6EMfH92Eumb1hasRhSF1RTUKsbpPefPUiNn0xVyWPLf1YFvRJZCMej0rTAmwDzgS2AmcD95nZSc65Lf4NzexdwKeBJcDLwH8CX0zdJyJV0HSBbpiWH8qXl+tvSH81NIsrWlppZWh0w9TIw5C6opqEVF0vIuOrF+1QXtpEUCeRjXw8KpJz7k/Act9dq8zsRbyva0vG5pcAtzvnugHM7DpgJQp0Raqm6QLdMC0/lG/ijL8hXR+fxaaz7mHW3mdGN0wVDkOGpXdbJApCkVNfDUGdRCotYhQzmwIcB3RneXg28EPf7WeAKWY22Tk3YukdM7sMuAxgypQpdHZ2Dj/W398/4naUaV/CK6r703SBbtgu55tr4kxmEDxrWhteyleGCoYhw9S7LRJ2hXLqhzVzr6Zy60cws1a8Hto7nXMbsmwyAXjDdzv974nAiEDXObcCWAEwf/5819HRMfxYZ2cn/ttRpn0Jr6juT9MFupFYfiilqNnj/mHI8ZP3r3VbRMMapt5tkWzCNOJQdH1p5l5N5dYPM7MYcDcwAFyeY7N+4CDf7fS/dwVYNJGaqvdxvOkCXajd8kM1k25MSuxFClvvtohf2EYciq4vzd6rqdx6zMyA24EpwNnOucEcm3YDc4H7UrfnAq9lpi2IRFUYjuNNGeg2pDJ6kaLUuy3NJ2wjDkXXl4B7NevdOyJF+XfgBGCpc25Pnu3uAu4ws5V4qy5cA9wRfPFERgvi2BKG47gC3UZRZi9Sw/VuS8MI44hD0fUloF7NMPSOSH5mNg34O2Af8KrXuQup+1YDzwInOue2Oud+bGY3Ao8D44EHgS/UvtTS7II6toThOK5At1EoN04aTMOPOJSxMkMYekckP+dcD2B5NpmQsf3Xga8HWiiRAoI6toThOK5ANyB1GV5skNw4Dc1KWsOOOJS5MkMYekdEakVtQe0EeWyp93FcgW4ANLxYnq6ePh58upcHunoZSuizkwZW5soMYegdEakFtaPFyXcyUMqJQiMfWxTolqCrp4/vP92LA94/b2rOH4KGF0u3sS/BTT9bw77BJC51nz47aVgVrMxQ794RkVpQO1pYvpOBck4UGvXYokC3SF09fZx/q/ejAXjgqW3ce9npWX8UGl4s3YYdCQaG9ge5BvrspHEpp14kL7WjheU7GdCJwn4KdLPI1t2/ZvN2BlNBLsBgwuX84bRPa+Pz58zm4fWvcNacw5v2x1WKWZPijGlJMDiUJB4zzpt/FOfm6TUXibwGyakXCUIjD6VXS76TAZ0o7KdANyUd3LYdMIZrV3WP6u5fNGMyrS2x4R7d1rjl/OF09fQNv8a6LTs4/rCJoamkYU3un9kW10FN6iKsdSIq/MfOvt0D+hylahp1KL1aMk8GAG5+fONwHczVpjbbMa9goGtmY4FvAkuBScAm4DPOuYcDLlvN+HNZYmYkkg7HyO7+9mlt3PvhRZHO0Q17cn/6c+7q6RtRWUVKVWw+fdjrRNilc+vTxzsDxrbqcxSpFX+7me1Ylm2CWrMd84rp0W0BtgFnAluBs4H7zOwk59yWAMtWM/7A1DmHGZgbnSNa7NllWIcMwhqA+zVjJZTqKiWfPgp1IszSufXJVHJ9ZgeBiNRGsceyZjzmxQpt4Jz7k3NuuXNui3Mu6ZxbBbwItAdfvNpIB6Yx8w7UzkEsZnz+nNll/QDSQwaf+MvjQxWopfczbvkneqV7VLt6+mpcwuyVUKQUufLps1k0YzIt8RgGxOPhOSmNCi+33jt2gteghOnkXqRZFNu+F7tdNhv7EnWLDSpRco6umU0BjgO6szx2GXAZwJQpU+js7Ky0fDVz5bwx/GDjAOu3ezP/k0nH0+s3cMSezSW9Tn9///B+zzbY9WIvnS9Wv7zlunLeGDbsSDBrUpxdLz4zqmwb+xLcuG4vg0lojcEnF4xjZls88HKlP7exOxO0GAw5iBuM3dlDZ2dvzudt7EsM708tyin1lZlbli3XrJR8esA7s/X/P0TCnkvnz61Xjq5I/aQ72B58ujfvZfnKneTX1dPHjev2MuSej9xoa0mBrpm1AiuBO51zGzIfd86tAFYAzJ8/33V0dFSjjDXRAZyaGjZPpxycv3RByV9kZ2cnYd7vjgKPdz++kSH3PA5IONh3yDQ6OmYGXq7059YBnDovf+Pun/xy08/SEwcTkap4QWnknPrMtJbPnzM768TRUvPph1I5+Ylk7pVU6sGf/xrmhsWfI6gRGAmLsJ8kBuX7T/cyMJTkwad7cx4zypnkt2bzdgaT0UxPKjrQNbMYcDcwAFweWInqyH+m03bAmOGDdlS+zJJsW5t1Dc9q5xdnHmzu+fXW4WXXLlh4dNbn5KuExUwcbHINm1Ofmdby8PpXcuaaRT2fHkbmv4b9963cegmTbCfFzTDaEGT+7aIZk2mNeR1gYTtWFlJUoGtmBtwOTAHOds4NBlqqOkr/KBr6oL1tLdz5V/uvynTJQ8PBbrXWLsx2Od9lp0/nlie8VJDVL/wRIGewm4u/IoM3cdA5MCNSFS8ozrk/Act9d60ys3RO/ZZ6lKlaMoPS2YcfxJObtgOu7ANvmNfq9K8tXWj/6t171YwTXCS8/L/HgaEkn//hepLONWZ77hPkiXv7tDY+uWAc+w6ZVnC0NWzH0mJ7dP8dOAFY6pzbE2B5QqEaB+2wfuGA15ObGACX8P6/ZfWIXt1K1y6859db+fwP1zOU3J/zODiU5Mfdr47Y7uH1r5Qc6PorcixmDCa89xhKwvOv7grfZ11n9cyp9+erV0s6x3xCq/HtX2xmKOnlcn/w2JasOefFli+M+fSHte7hynnj8+bUQ/3y6iH8ufVB/AYl/PzthDXRyF/QJ+4z2+I5UxnDPKpTzDq604C/A/YBr3qduwD8nXNuZYBlq5tKz4pq9YWXHUxPX+z15KZ7dKcvLvl9s+U/pntxv7duGwlfkJu+nO+7Zx823KMLcNacw0t6XxhZkX/S/SrP9L4x/Fg5gXMjq3dOfRD56ulXu/nxjQw99/zw/W856piSc8nDnk/f2dnJpUWUr1559VBabn1aV0+fL/c42Nz6sH/HEozMNMRrV3WHMj0pCPW6yEaYR3UKBrrOuR7IO4mv4VR6VlSLL7yiYPqo07x0BV+ObrFBc+Yapfet28r3/u7PAC/dY9+gt2pFWtzgg6cdPXw536MnH1gwR7eQdEVuO2AMz/T+fvj+cgLnRtXoOfVhzq0NTJ3y6os9NhTbwIa5QZTG4f89Hn/YxPCOsDaIMB+TdQngHCo5K6rFF75m8/bhoHJgsPjGYn+jdSzti08bvi9f0Oxv6NKNVNpQEh58upcjDxnPwND+INeAeMy49r1zuGDh0SOudlZqgJuroU2/TqWBc6Nphpz69MloemSh4dUgrx6KX9miEotmTCYWM5IJh8UKLP0WUWZ2ObAMOAm41zm3LMd2y/Dqqj8l8BznXGewJWwuupRw8MI830GBbgAKfeGl9J7m2q7tgDHDDXwydbuQrp4+zl/xJIMJR2vchq8Wla+HJVvDF4/ZiNSE7pfeYM4RBw8H9/GYcd78o5h9xMH07R7gnl9vzdpY+vcvX5nzBeEXLDy6rAA31DnUlWuanPoHU0vpfD/PUjoNIeC8evDqxL88+t9Fr2xRrudf3cVQOrc+4Ro1t/5l4HrgXcD4Ats+6Zz78+CLJBJs2xfWEwoFuiWqdBiv2JSDQtv17R4gZpB0EDPvdiEPPt3LQKqBGUg4Hny6l/ZpbVl7oNP7+dLOPSMaur7dA1z33jl87ofrh4PdZ3rf4LlXu1n+nv1LuAAjlgFLOjfqamf+/bty3pisa/z6g/C9g0m+9fNNrLh4fsF9zSfMSfOVaqac+qYaAq8wr76QzAmksdRVk86aczjrtuyo6ujUw+tfGXW70UZjnHPfBzCz+cDUOhdHBGjsti8fBbolqMaPpFrXoy4nPSIz0Tp9O7MHGvYHoS3xGC2pHtz0+7RPa+P4wyZy7f/rHp4MNjCUpPvlN/jS+04CvMlCw8uAOUcsZlhqGai2A8aM6jn65UtD7EulNmTuZ3qYE+Anz77GPb/eygVHvJo1X7EYjRwgNVNOfZhzwrKpqCclS159qe+dvmLSuRkX0Ojq6RsR5BpwxsxD+fjS44brejV7gM6ac/jw8oLp203uVDP7I7ADL6/+y865ocyN8q2S0kirS2hfgrNq08CIlMd7H13HrrcVHg1OC9v+FEuBbha5GqRqBEjFNs6Ftis1HyZ9bep4DBJJ77Ko587b39Hg74H2B6mJRJIPnnY0Rxwyfvh9Nqx7lIFnH2PRmON5hkOHX+P1XftGlL8ltfxXS9z4X2ccQ/crbzL78IO4dlX3cGWLGcTjMVa/NMTPXxp9acH2aW3MPvygEasrvND1M/jpZ7LmKxYjagGSZBfmnLBM5Z4kjzwWnTb8Oy8UNGemBZ2/4snh0Zz7u3q598P733/N5u0kfZc/jsdsOMiF6g9HKrd+hCeAOUAPMBv4HjAEfDlzw3yrpDTS6hLal+BMPKaPVVvKv/pr2PanWAp0M+RrkKoRIGVrnLM1WsU04sU2QP59SqfW5uvyaztgDLHUlRhaW2IjeoA2rHuUaavOZyZDnEILXfHP8lTiOAA6n/8DXT19+8tkBnjrF377ly8ylHQ8uWn78JqGMbyeo6MmHcC9v96KA/YNevmW/v36mwVHj1hd4b2HvAiv585XLKTY70DCL6w5YZnKmTya6xLAxUwe9T/+/nlTh9ebhtEn6enjWjrN6Nr3zqlKncj3/HJz6xuNc26z7+bvzexa4CqyBLoilYpS50A1KdDNkK/XttwfSeYB39+T8vyru4YnarXEY3ygferw2rTlNOLZGpeRVxPzJJIua2Pb1dPHtau6SabSDT5/zuyRucHPPsZMhmixJLgh/urgF+nacZy3hqfvNdds3s5QwmvY0xNP0v+NxwyXCqI/vtQLku9bt5Wh1HW0739q24jgOrMH6JQj3gKbV1SUr+j/bJs1b0kKKyXYq/bk0Q07ElmD40IjS5mPO7wRnHSPbuZJeq4Tv2x1opSJtEHVqQY/KXU0SeqR1EdUOgeqSYFuhmJSBkr5kWQ74EP2iVoDQ0nu/fXWsmeQ52pc/D02Sef1pObqkfY3koYbNcmt7cQlDG6+FdwQg7Rw2MlLGfvL0Z+X/3OMx2MknSORcLTEYyMmraX3cfERcTp7E6MC5vR+9e0e8A2nHl1RvmK+fW60nN1mVo0eyWKDtSAmj05otazBca5jVHp/2w4YM+Lx98/zTp7TObqzjzh4eEJorvSEbHUCRl8aPZcgJpGm9zEKJ6Vm1oLXvsaBuJmNA4Yyc2/N7Czgaefca2Y2C/gccH/NCxxSDX5SIzWiQDdDtbv2czUYmRO1nPOG8yu5TGGugM2/T20HjBkVZPoVCvRnLVjKBu6l79nHaDtxCX+5YCkrT8ifetF2wBiWP7SeBN7+Hn/YxFHvfcaRrTz5msvaeGdt2I46reIAt9h9luip5cTRYrYt5zfWP+iyBsfF9MB+/pzRJ5PFpD3kK+/3n+4d7mFO7+PsHH2Pi2ZMJl7lSaQQqZPSa4Av+G5fBHzRzL4NPAuc6JzbCrwTuMPMJgCvAd8Bbqh1YcMoKic1En4KdLOoZtd+rgbOf9/nz5lN98tvcP9T27xhfjN+u23nyHzXDNnOdPM1psXuUzGB/qwFS2HB0oKvnb7vXx79bwYTLmtvbdrMtnjW961Fw9aseUuNIAwTR4vZtpzJo9v3JL2T4ISjJePCCoV6YPt2D/AP79h/GeAN6x6l79nH+G38JAaGDsr5ufg/T/8FOZ5/dRf3P7VtuIc5Hvf2cdeLvVnL3z6tjROrPIkUonNS6pxbDizP8fAE33ZXAlfWoEiBCLLHNUInNQ2hkXvPIxnohvUL6erpY9WmASYesz9AzdXAZbtv9hEHD69P+9NnX+Pnz/9h+KIOme+T7Uy32Ma00OdXrUA/vTanf/JZPGa8vHNP1iA+2/vWqmFrxrylqKvHxNFKti32N5ber72D+69AyP71kLMankCKG7W/mRNIH49fTVfi2FHbZesVTl+QI51iBV4C6QfavRz6zhdzH0+qPYkUsi+FeHOWZQkleEH3uEblpKYRNHrveeQC3Xp/IbkO6uly7RtMsmrLmlFLZBUT1PXtHiDpmzE2mMje+1lowly9Jolkvk/m2pwnTT2Y517dxb1rt/JggTzkzJ6lMJ7YSH0FMXEUsk8e9V/COtdrlXOylG/yqF8ikbtHKz2BNJH0JnoWmkD6t1Nf5szjzynYC+6/KlrmWtjvTy1NmGtlCAhmEikw4jtp5MY57ILucdVIW+00eu955ALden4h+Q6s6XLlyrEtphd60YzJtKYmjYE3UzrbWWwlZ7q1+vzWbN4+4jLB8Zgx58iD+f1LbxR872yfs38YVgSqP3EUCk8erWZAVWjyqH+t6Xz13H/sca7wBNJp7X/JuxaMrk+Zn2fmVdGy5f1u2JHIezwZuYyYJpE2klr0uGqkrTYavfc8coFuPb+QfAfW4ZUNBkeXq9ieh/Zpbdz74f15ce/PuIKRf7tyz3Rr9fm1HTCGeOqKavGYtzbn8YdN5MGnewu+txowKUYQPT6FJo9W8/dYaPLovY+uY96cWXknj0LpE0hn+fLr/XJ9nvku7DBrUpwxLYnijyeaRNow1OPaOBr9u4xcoFvPL6TQZK9045R5tZFcjWe2fShl0lg5+16Lz8+/Fm86yE03ksW8txowKVa1e3yKmTyauZxXuWvsFjqe7HrbGDqKuKhCMXU6cwJpvteC0Wt8r9uyI+tqKbkmkdZCozfOUaAe18bRyN9l5AJdqN8XUujAmm6c0vlj6e0yG7S2A8bUNbcs6M8v31q8xby3GrDGsLEvQXdIJwplmzgKxU8e9Y/SZF7oJdt7BTl5NF3uaqdT+Nf43jeY5MGMKxZW+73L0ciNs4hURyQD3SBVY0WCbA2bv0ErZ2jevxh8oaFMtq2tWh5cOfL1VBXbC6YGLNq6evq4cd1ehtzzoZsolG/iKBQ3edRfhwtd6CUKk0ezlRXnhhd7cMADXb05g3l/eXWCKmGg36KkKdD1qVajktkIrtm8nX94x8wRr1XK0Ly/XF4vKYxtzVG+bWvhzr+qaK3KSuXqqdIs6eaxZvN2BpOVXQClUoXW2K104qh/wli+/YzC5NFcZX37sW/hp8++5q2BnWflB1D9lvDQb1H8FOj6VKtRaTtgDOkFB5Ju5HXt041othnM+co1O7GBhbHnWJM8gafdcbnLt2W1F+RWsFZlNWTrqdIks+axaMZkWmOQcPlXDAhKMWvsVjpxNH1Bhfuf2kYiOXr92sxtwzx5FLIfmwCeeOH1ot5f9VvCIltnk36LzUuBrk+1GpW+3QMYDF8kIZ2jWu5Z5jsnbOF/td5AK97yQBcNfJbullnZyzd9sdeTW+FalUHQJLPm0T6tjU8uGMe+Q6bVZeiwmDV2i504mplzn5l2cO68qYHl0NYqXz3fsanY91f9lrDI19kkzUeBrk+1GpVFMyYztnX0Ab/kHo9Uru2sN3pxsSHMJYmR4MpZrzOmY1n25x51WlXXqixVvmFfTTJrLjPb4nR01Gf942LW2E1PHC30vEInqEHnk9ciX72SPGJ/OVW/pZrKzbPN1dkkzUmBboZqNCq5Dvgl9Xj4c21jcSzWCskhYvExnL7kr+GoPGWs4lqVpSimx1qTzKQWyg26sj3v5sc3lj0kX1RDXefJo1C93ljVb6mWckdAu3r6eHnnHlrjljelSJqHAt2A5DrgnztvKpb6v3+S1qjG0J9rmwTaL4aDj6prY1iIcvQkTIoNurJd8tf/vHKCwK6ePh58upcHunoZSuRpqEMweRTUGyvhU+7qRP5l//7mtKMKrhQijU+Bbo1knp2em7pWfFdPH+eveJLBhKM1btx72elepczMtZ17QWgD3DTl6EnUFDsKUUoQ6F++LH0R7LBPHgX1xkq4lNOe+IPjRCLJkYeM129aFOjWSq6z0wef7mUg4TWHAwm3f1H2OufalkO9QhI1+XqNsvX0FvuasxMbWBj3Vkn5jTsud0Md4smjIvVUTnuizhbJRoFuQDIbyVwV0DKeN+J2nXJtK6FeIYmSXPWyknU4M1dJufPYf2PB4neHcvIoaGH9IJjZ5cAy4CTgXufcsjzb/iPwKeAA4AHgI865fTUoZuiV2p6os0WyUaBbBf6GIn07WyOZrQJOHDvyK5h9xME1L79Is8pVL8vKNx9eJWXb8CopcUvwv6e9DPmeW8cTWi2sH5iXgeuBdwHjc21kZu8CPg0sST3nP4Evpu6TMoS9s0UnlrWnQLdCmQ3FlfPGsM9lbyQzK2BXTx+3/eLFEa+nZVBEaitbw1jyEOiIVVJasFgLJBNYyNMRNIE0GM657wOY2Xxgap5NLwFud851p7a/DliJAt2GpBPL+lCgW6HMhmLDjgTnzyuukVyzeTtD6VWtU7SwtUj9tU9r4/PnzObh9a9w1pzDCzdGo1ZJuQQOnhr6/HrlNNbdbOCHvtvPAFPMbLJzbnudyiQB0YllfRQV6JaSb9RsMhuKWZPiRecJLZoxmZaYDQe7Ru16dDV8IpJbV08f167qZmAoybotOzj+sIn568moVVLOD3WAm6acxrqbALzhu53+90RgRKBrZpcBlwFMmTKFzs7O4cf6+/tH3I6yRt6XsTsTtBgMOYgbjN3ZQ2dnb/0KWKKofjfF9ugWlW/UjDIbil0vPjN8f6FGo31aG5f++TGsWL0Z52Bsa216VDR8IpJfKasxAKGYVFausOc0Nrh+4CDf7fS/d2Vu6JxbAawAmD9/vuvo6Bh+rLOzE//tKGvkfekATp0X3U6mqH43RQW6JeQbNSV/Q9H5YoGNfbp6+rjjyS04B/GY8flzZpe1wH2pNHwikl9ZqzFEcJUUqbtuYC5wX+r2XOA1pS00Lp1Y1l5Vc3TzDa00i1K69ldtGhheVH4o6fjpuuc4Ys/mvM/Z2JfgxnV7GUxCaww+uWAcM9viJZWx2OGTjX0JNuxIMGtSvOT3KFVUh0QketInimN3JujIsU1VV2OQpmNmLXjtaxyIm9k4YMg5N5Sx6V3AHWa2Em/k9BrgjlqWVaTRVTXQzTe00gy6evpY9eg6zj9pblGN38Rj+njoRa93COCXryS4/D35n9v9+EaG3PM4IOFg3yHT6OiYWVI5Oyg8fNLV08dNP0v3XCUCT2+I6pCIRMuIS4SaVw9y/a5zrcbQEvd6euNxTd6SnK4BvuC7fRHwRTP7NvAscKJzbqtz7sdmdiPwOF5a4IMZzxORCsXqXYAo6urp4+bHN9LV0zfivgtvW8ODLwxy4W1rRjyWS/u0Nj7QPnX4IhGJpGPN5vwjVukh1bhR8ixpf7nbp7XxD++YmbORz9ZzJdFgZpeb2VNmts/M7qh3ecLE/7seSlLe79q5kf8XyeCcW+6cs4y/5angdoJzbqtv268756Y45w5yzv1PXSxCpLq0vFiJcuXopRtQR2lDmu+fN5XvP91b9PI+5c6SLnUCmpYdijRNHs3B/7uOGyX/rtNLAjr2n5gqdUFEJLyKXV6s2HyjhpcrRy/dgA4MlhYYlhO4lpPMXmpuoZYdii5NHs3N/7seu7OnpN91V08fL+/cQ0vMSCRd0fVcS/mJiNRPsT26WfONgOXVLlDY5erpTDeg9z66jvOXLij5+txBN4Dl9NBqdmhjC3ryaJgnGM426G/dU3T5/JNA4zF4+5FxzjiylV0vPpN3pZVKJo+G+fNLC3sZw14+EQlescuLLacJg9ps8vV0tk9rY9fbxoQyOFQPrWQKevJo2CcYllI+/yTQZBKGxrdx6rzjCtajSiaPlvP51br3uJG+YxFpTMrRLUNUezqjWm6Rauvq6WPVpgEmHpN71QU/f2pSEvjlxj+ybsuOUOW660IwIiKjNd2qC9lWTBCRxlTNFVJWXrqIM449lJhR9Gok6ed94i+PLyvwLOV4pZVSpFrUTkojaaoeXfV4SDPQ5FFPtVdIaZ/WxseXHse6LTtqkuu+sS/hW8taK6VIbaidlEbTVD266vGQJnENsAf4NN7E0T2p+5pKrvqeDghjlL4WdaU9tKXYsCNR0vGqlmWTxqV2UhpNU/XoqsdDmoEmj3qCWCEl/fxaBJGzJsUZ05LQSilSU2onpdE0VaCrlQdEmkdUV0hJm9kW1/FKak7tpDSapgp0QT0eIs0k6vU96uWXaNLvThpJU+XoioiIiEjzUKArIiIiIg1Jga6IiIiINCQFuiIiIiLSkBToioiIiEhDUqArIiIiIg3JnHPBvLDZ60BPIC8ebocCf6x3ISKoGT63ac65t9S7ENkEVF/D/p2qfJULexkrKV+U6mvYv4dSaF/CK8z7k7O+BhboNisze8o5N7/e5YgafW6NJ+zfqcpXubCXMezlq5ZG2k/tS3hFdX+UuiAiIiIiDUmBroiIiIg0JAW61bei3gWIKH1ujSfs36nKV7mwlzHs5auWRtpP7Ut4RXJ/lKMrIiIiIg1JPboiIiIi0pAU6IqIiIhIQ1KgKyIiIiINSYFumczscjN7ysz2mdkdvvsXmdlPzWyHmb1uZveb2eF1LGqo5PrcMrb5vJk5M1ta4+JJGfLUhTFm9oCZbUl9nx0hK18o6mqe8p2Yur8v9feomZ0YlvJlbFPXOpvnM5yeKle/7+9z9ShjNYS9rpUi7PWyFGGvw6WIQn0vlQLd8r0MXA98O+P+NryZidOBacAu4D9qWrJwy/W5AWBmbwPOA16pZaGkIvm+018AFwGv1rREI4W9ruYq38vAB4BJeFckegj4bm2LNlyOsNfZvGUEDnHOTUj9XVfDclVb2OtaKcJeL0sR9jpciijU95K01LsAUeWc+z6Amc0Hpvruf9i/nZl9A/h5bUsXXrk+N5+bgU8B36xluaR8eerCAPAvqccSdSkc4a+recq3E9iZesyABDAzLOXzqXudLaKMDSHsda0UYa+XpQh7HS5FFOp7qRToBu/tQHe9CxEFZnYesM8591/eMUGkpkJZV81sJzABbwTu8/UtzUgRqrM9ZuaAnwJXOef+WO8CSdFCWS9LEeY6XIoI1fcRFOgGyMxOxvtRv7feZQk7M5sI3AD8Rb3LIs0nzHXVOXeImR0IXAL01Ls8aRGps38EFgC/BSbj9UatBN5VxzJJkcJcL0sR1jpciojU96wU6AbEzGYCDwMfc86trnd5ImA5cLdzbkudyyFNJgp11Tn3JzO7BXjdzE5wzv2h3mUiAnXWOdcPPJW6+ZqZXQ68YmYTnXO76lg0KSAK9bIUIa3DpVhOyOt7LpqMFgAzmwY8ClznnLu73uWJiHcCHzWzV83sVeAo4D4z+1SdyyUNLGJ1NQYcABxZ74KkRLHOpi8FqrYvxCJWL0sRtjpciijWd0A9umUzsxa8zy8OxM1sHDAETAEeA77hnLuljkUMpTyf2zuBVt+m64BP4J3RS4jl+k6dc0NmNhZIJ3ONST22z9Xw2uNhr6t5yvcOvKH33wEH4s2E7gOeC0n5QlNn85SxHW8y0At4s/n/Deh0zr1R6zJWQ9jrWinCXi9LEfY6XIoo1PeSOef0V8YfXje+y/hbDnwh9e9+/1+9yxuWv1yfW5bttgBL611e/VX2naa+x8zHpoehfGGpq3nKdx6wIVWu14EfASeHpXxZtqtbnc3zGZ4PvAj8CW85pLuAw+pRxqC/izDUtSp9Z6Gol1Xal1DU4Wr9xjK2q1t9L/XPUgUWEREREWkoylMSERERkYakQFdEREREGpICXRERERFpSAp0RURERKQhKdAVERERkYakQFdEREREGpICXRERERFpSAp0RURERKQh/X+j/ORaZNPQAwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 864x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_data(X, y):\n",
    "    plt.plot(X[y==0,0], X[y==0,1], '.', label='class 0')\n",
    "    plt.plot(X[y==1,0], X[y==1,1], '.', label='class 1')\n",
    "    plt.axis('equal')\n",
    "    plt.grid('on')\n",
    "    plt.legend()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,3,1)\n",
    "plot_data(X, y)\n",
    "plt.title('All data')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plot_data(X_train, y_train)\n",
    "plt.title('Train split')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plot_data(X_test, y_test)\n",
    "plt.title('Test split')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Randomness and feature scaling in Stochastic Gradient Descent\n",
    "\n",
    "We'll explore how the randomness and scaling can affect the result of a classifier trained using SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h1>ðŸŽ¥ <a href=\"https://brightspace.tudelft.nl/d2l/le/content/682421/viewContent/3726352/View\">Lecture 3A - Parameter optimization</a></h1>\n",
    "    The following requires the knowledge covered in this lecture. If you haven't watched the video yet, it's now high time to do so...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_eval_stats_on_data(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    print('  accuracy:', accuracy_score(y, y_pred))\n",
    "    print('  confusion matrix:')\n",
    "    print(confusion_matrix(y, y_pred))\n",
    "    print('  f1:', f1_score(y, y_pred))\n",
    "\n",
    "def print_eval_stats(clf, X_train, y_train, X_test, y_test):\n",
    "    print('TRAIN')\n",
    "    print_eval_stats_on_data(clf, X_train, y_train)\n",
    "    print()\n",
    "\n",
    "    print('TEST')\n",
    "    print_eval_stats_on_data(clf, X_test, y_test)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the interactive widget below, we can see how the `random_state` parameter of the SGDClassifier affects the classification result. \n",
    "The parameter is used when determining the randomness in the process, such as the random initialization and the order of how the samples are processed for gradient computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ec094db96f41b582da2b0c061fbb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='random_state', max=49), Output()), _dom_classes=('widgetâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.eval_sgd_random_state(random_state=3)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_sgd_random_state(random_state=3):\n",
    "    sgd_clf = SGDClassifier(random_state=random_state)\n",
    "    sgd_clf.fit(X_train, y_train)\n",
    "    \n",
    "    print('random_state =', random_state)\n",
    "    print_eval_stats(sgd_clf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "ipywidgets.interact(eval_sgd_random_state, random_state=(0,49))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Write some code to do the following:\n",
    "1. train the standard `SGDClassifier`, and set its random_state to a random seed. Don't set any other keywords in the constructor, just use the default values.\n",
    "2. evaluate the classifier on the test data\n",
    "3. compute the accuracy and F1 score on the test data.\n",
    "\n",
    "Use this to compute for the first 50 random seeds (0 up to 49) both the mean and standard deviation of the two metrics on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95828e4ff45419dc449ac942e1d3402a",
     "grade": false,
     "grade_id": "cell-72d38fa04335aec1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: average = 0.749\t(std.dev. = 0.122)\n",
      "      F1: average = 0.358\t(std.dev. = 0.235)\n"
     ]
    }
   ],
   "source": [
    "random_seeds = range(50)\n",
    "accuracies = []\n",
    "f1s = []\n",
    "for seed in random_seeds:\n",
    "    sgd_clf = SGDClassifier(random_state=seed)\n",
    "    sgd_clf.fit(X_train, y_train)\n",
    "    y_pred = sgd_clf.predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    f1s.append(f1_score(y_test, y_pred))\n",
    "accuracy_mean = np.mean(accuracies)\n",
    "accuracy_stddev = np.std(accuracies)\n",
    "f1_mean = np.mean(f1s)\n",
    "f1_stddev = np.std(f1s)\n",
    "\n",
    "print(f'Accuracy: average = {accuracy_mean:.3f}\\t(std.dev. = {accuracy_stddev:.3f})')\n",
    "print(f'      F1: average = {f1_mean:.3f}\\t(std.dev. = {f1_stddev:.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3cd7773281917fbf80bedb4422b409a1",
     "grade": true,
     "grade_id": "cell-48f7e4cc5445d133",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "assert(isinstance(accuracy_mean, float))\n",
    "assert(isinstance(accuracy_stddev, float))\n",
    "assert(isinstance(f1_mean, float))\n",
    "assert(isinstance(f1_stddev, float))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the randomness in the SGD optimization can have a significant impact on the results.\n",
    "\n",
    "**Q**: you should see that the accuracy is generally pretty good, but the F1 score is quite low and shows more variance. Why is this the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dfa58bba636b2229b10b2787701bd63f",
     "grade": true,
     "grade_id": "cell-646846ada363512c",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Class 0 is overrepresented compared to class 1, and the model predicts class 0 more often than class 1. So, the accuracy, or the relative amount of true positives for either class, is relatively high, but the f1 score is low, because it compensates for the imbalance in classes. The variance in it is high because a different seed will result in a different class imbalance because the total dataset is quite small. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Next, let's take a look at what happens when the axes in the feature space are scaled differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd82041bac241cd1e3a8f64927902130",
     "grade": false,
     "grade_id": "cell-340eead4bae76cef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create a version of the data with scaled and shifted features\n",
    "X2 = X.copy()\n",
    "X2[:,0] *= 4. # scale one feature\n",
    "X2 += np.array([10., -3.0]) # shift both features\n",
    "\n",
    "X2_train, X2_test, y_train, y_test = train_test_split(X2, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b547a77d506636f52e57bc5f60bbb75",
     "grade": false,
     "grade_id": "cell-6878f92d66c58d27",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Normal\n",
      "TRAIN\n",
      "  accuracy: 0.7535211267605634\n",
      "  confusion matrix:\n",
      "[[98  4]\n",
      " [31  9]]\n",
      "  f1: 0.339622641509434\n",
      "\n",
      "TEST\n",
      "  accuracy: 0.8055555555555556\n",
      "  confusion matrix:\n",
      "[[28  0]\n",
      " [ 7  1]]\n",
      "  f1: 0.2222222222222222\n",
      "\n",
      "** Scaled\n",
      "TRAIN\n",
      "  accuracy: 0.7183098591549296\n",
      "  confusion matrix:\n",
      "[[102   0]\n",
      " [ 40   0]]\n",
      "  f1: 0.0\n",
      "\n",
      "TEST\n",
      "  accuracy: 0.7777777777777778\n",
      "  confusion matrix:\n",
      "[[28  0]\n",
      " [ 8  0]]\n",
      "  f1: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE: using the same random state here\n",
    "\n",
    "# original data\n",
    "sgd_clf = SGDClassifier(random_state=3)\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "\n",
    "# scaled data\n",
    "sgd_clf2 = SGDClassifier(random_state=3)\n",
    "sgd_clf2.fit(X2_train, y_train)\n",
    "\n",
    "print('** Normal')\n",
    "print_eval_stats(sgd_clf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print('** Scaled')\n",
    "print_eval_stats(sgd_clf, X2_train, y_train, X2_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a function that use sklearn's `StandardScaler` to standardize the given data.\n",
    "The scaler can determine the mean and standard deviation of the features in the data you fit it on.\n",
    "Afterwards, it can be used to transform given features, removing the fitted mean, and scale the dimensions such that each has std.deviation of 1.\n",
    "\n",
    "Note that fitting the scaling is considered part of the **training** process. Consider this when you implement the function: How should the training and testing data be scaled? What can you say on the resulting mean vector of both datasets after standardizing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ac4fdcec056f95a063c94edb9ae439f",
     "grade": false,
     "grade_id": "cell-f94b8b75454b09d7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as pp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardize_my_data(X_train, X_test):\n",
    "    \"\"\" Use the StandardScaler() to standardize the data. \n",
    "        Returns the standardized versions of the given training and test data.\n",
    "    \"\"\"\n",
    "    sc = StandardScaler()\n",
    "    X_train_standardized = sc.fit_transform(X_train)\n",
    "    X_test_standardized = sc.transform(X_test)\n",
    "    \n",
    "    return X_train_standardized, X_test_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean vector of the training dataset should be 0, but the one of the test dataset will not be exactly 0, because the scaler uses the means of the training set to scale the testing set. And with this small amount of data, the likelihood of the testing set having the exact same mean as the training set is very low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e960e7c138a13cb7c333470cc267359",
     "grade": true,
     "grade_id": "cell-54cec714b358ea5a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "  accuracy: 0.7394366197183099\n",
      "  confusion matrix:\n",
      "[[93  9]\n",
      " [28 12]]\n",
      "  f1: 0.3934426229508196\n",
      "\n",
      "TEST\n",
      "  accuracy: 0.7777777777777778\n",
      "  confusion matrix:\n",
      "[[27  1]\n",
      " [ 7  1]]\n",
      "  f1: 0.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# standardize the data using your function\n",
    "X2_train_standardized, X2_test_standardized = standardize_my_data(X2_train, X2_test)\n",
    "\n",
    "# retrain the classifier for X2\n",
    "sgd_clf2.fit(X2_train_standardized, y_train)\n",
    "\n",
    "# evaluate\n",
    "print_eval_stats(sgd_clf2, X2_train_standardized, y_train, X2_test_standardized, y_test)\n",
    "\n",
    "# test that the training data is indeed zero mean\n",
    "assert(np.all( np.abs(X2_train_standardized.mean(axis=0)) < 1e-10 ))\n",
    "# test that all the features in the training data have std.dev. of 1\n",
    "assert(np.all( np.abs(X2_train_standardized.std(axis=0) - 1.0) < 1e-10 ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the results after scaling the data again resemble the earlier results on the original undistorted data, though they may still be a bit different since the original data wasn't standardized either.\n",
    "\n",
    "Again, even with the same random seed, the scaling of the feature space affects the solution that SGD finds.\n",
    "Standardizing the data to ensure all feature dimensions are in the same order of magnitude can help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reimplement binary classification metrics\n",
    "\n",
    "In the practicum assignments we have now used various binary classification metrics, for which sklearn provides builtin solutions for your convenience. Still, any ML practicioner should have a good understanding of what these metrics represent, and should be able to compute these themselves without relying on sklearn.\n",
    "\n",
    "In this section, reimplement the common binary-classification performance metrics yourself.\n",
    "Many metrics can be expressed in terms of the number of True Positives (TP), False Positves (FP), True Negatives (TN) and False Negatives (FN) that your classifier obtained.\n",
    "\n",
    "### 4.1 Therefore, first implement a function that computes these four numbers from the groundtruth labels and your classifier's perdicted labels.\n",
    "You can assume that y contains integers representing the two class labels, and you should consider the largest label in `y` as the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "456fa1db25d75122f37c31ba3ebffdb0",
     "grade": false,
     "grade_id": "cell-96fddabdd349a1c6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def my_tp_fp_tn_fn(y, y_pred):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return (TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f77fab2522816cd0031e379845287dc7",
     "grade": true,
     "grade_id": "cell-1b6fdc9405ef5052",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test on some fake predictions and GT labels\n",
    "y_dummy = np.array([0,0,0,1,1,1,1,1])\n",
    "\n",
    "y_pred_dummy1 = np.array([0,1,0,0,0,0,1,1])\n",
    "TP, FP, TN, FN = my_tp_fp_tn_fn(y_dummy, y_pred_dummy1)\n",
    "assert((TP, FP, TN, FN) == (2,1,2,3))\n",
    "\n",
    "# all correct\n",
    "y_pred_dummy2 = np.array([0,0,0,1,1,1,1,1])\n",
    "TP, FP, TN, FN = my_tp_fp_tn_fn(y_dummy, y_pred_dummy2)\n",
    "assert((TP, FP, TN, FN) == (5,0,3,0))\n",
    "\n",
    "# all wrong\n",
    "y_pred_dummy3 = np.array([1,1,1,0,0,0,0,0])\n",
    "TP, FP, TN, FN = my_tp_fp_tn_fn(y_dummy, y_pred_dummy3)\n",
    "assert((TP, FP, TN, FN) == (0,3,0,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ca7f38539c5cc85b6f1eec6ed66fc4a",
     "grade": true,
     "grade_id": "cell-0169ea10918033fb",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Extra point if you make your function work with not only\n",
    "#    with labels [0, 1], but also with other labels, e.g. [1, 2]\n",
    "\n",
    "# Test on some fake predictions and GT labels\n",
    "y_dummy = np.array([0,0,0,1,1,1,1,1])+1 # <---- NOTE the +1\n",
    "\n",
    "y_pred_dummy1 = np.array([0,1,0,0,0,0,1,1])+1 # <---- NOTE the +1\n",
    "TP, FP, TN, FN = my_tp_fp_tn_fn(y_dummy, y_pred_dummy1)\n",
    "assert((TP, FP, TN, FN) == (2,1,2,3))\n",
    "\n",
    "# all correct\n",
    "y_pred_dummy2 = np.array([0,0,0,1,1,1,1,1])+1 # <---- NOTE the +1\n",
    "TP, FP, TN, FN = my_tp_fp_tn_fn(y_dummy, y_pred_dummy2)\n",
    "assert((TP, FP, TN, FN) == (5,0,3,0))\n",
    "\n",
    "# all wrong\n",
    "y_pred_dummy3 = np.array([1,1,1,0,0,0,0,0])+1 \n",
    "TP, FP, TN, FN = my_tp_fp_tn_fn(y_dummy, y_pred_dummy3)\n",
    "assert((TP, FP, TN, FN) == (0,3,0,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Using these numbers, implement the following metrics:\n",
    "- Accuracy\n",
    "- Confusion matrix\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "\n",
    "Notes:\n",
    "* Remember that these metrics were explained in Chapter 3 of the book.\n",
    "* Your implementation of these functions should all use the numbers returned from the `my_tp_fp_tn_fn()` function, except for the F1 score, for which you can reuse the results of the `my_precision_score()` and `my_recall_score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1dab54dc1be11707b596f99f5361493b",
     "grade": false,
     "grade_id": "cell-6a14ac4015a2dde1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def my_accuracy_score(y, y_pred):\n",
    "    TP, FP, TN, FN = my_tp_fp_tn_fn(y, y_pred)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def my_confusion_matrix(y, y_pred):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def my_precision_score(y, y_pred):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def my_recall_score(y, y_pred):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def my_f1_score(y, y_pred):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c730f658ab6c5fe7940c6f3970078a72",
     "grade": true,
     "grade_id": "cell-5365dce93fcd6dfb",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# get some classification results for reference\n",
    "sgd_clf = SGDClassifier(random_state=4)\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "y_train_pred = sgd_clf.predict(X_train)\n",
    "y_test_pred = sgd_clf.predict(X_test)\n",
    "\n",
    "# run defaul implementation\n",
    "sklearn_confmat = confusion_matrix(y_train, y_train_pred)\n",
    "sklearn_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "sklearn_precision = precision_score(y_train, y_train_pred)\n",
    "sklearn_recall = recall_score(y_train, y_train_pred)\n",
    "sklearn_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "print('*** SKLEARN IMPLEMENTATION ***')\n",
    "print(f' confusion matrix:')\n",
    "print(sklearn_confmat)\n",
    "print(f' accuracy: {sklearn_accuracy:.3f}')\n",
    "print(f'precision: {sklearn_precision:.3f}')\n",
    "print(f'   recall: {sklearn_recall:.3f}')\n",
    "print(f'       f1: {sklearn_f1:.3f}')\n",
    "print()\n",
    "\n",
    "# run student implementation\n",
    "my_confmat = my_confusion_matrix(y_train, y_train_pred)\n",
    "my_accuracy = my_accuracy_score(y_train, y_train_pred)\n",
    "my_precision = my_precision_score(y_train, y_train_pred)\n",
    "my_recall = my_recall_score(y_train, y_train_pred)\n",
    "my_f1 = my_f1_score(y_train, y_train_pred)\n",
    "\n",
    "print('*** YOUR IMPLEMENTATION ***')\n",
    "print(f' confusion matrix:')\n",
    "print(my_confmat)\n",
    "print(f' accuracy: {my_accuracy:.3f}')\n",
    "print(f'precision: {my_precision:.3f}')\n",
    "print(f'   recall: {my_recall:.3f}')\n",
    "print(f'       f1: {my_f1:.3f}')\n",
    "print()\n",
    "\n",
    "assert(np.all(my_confmat == sklearn_confmat))\n",
    "assert(np.all(my_accuracy == sklearn_accuracy))\n",
    "assert(np.all(my_precision == sklearn_precision))\n",
    "assert(np.all(my_recall == sklearn_recall))\n",
    "assert(np.all(my_f1 == sklearn_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decision boundaries in the feature space\n",
    "\n",
    "In sklearn, we can evaluate the continuous hypothesis function of a trained classifier using the `decision_function(x)` member function of the classifier object.\n",
    "Since we are using a 2D feature space, we can try to map the shape of this hypothesis function, and the resulting decision boundary, by evaluating these functions at a fixed number of grid points in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_space_function(X, y, f, res=0.1):\n",
    "    \"\"\" Plot the 2D feature space of the first two features of the data in X.\n",
    "        For the feature space, plot the samples in X with their class labels y,\n",
    "        and also overlay a countour plot with the function f(x) evaluated at a grid within\n",
    "        the shown region of the feature space (the size of the region is determined by the extent of the data).\n",
    "        based on https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html\n",
    "    \"\"\"\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, res),\n",
    "                         np.arange(y_min, y_max, res))\n",
    "\n",
    "    Z = f(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plt.plot(X[y==0,0], X[y==0,1], '.', label='class 0')\n",
    "    plt.plot(X[y==1,0], X[y==1,1], '.', label='class 1')\n",
    "    plt.grid('on')\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Linear classifier\n",
    "We will first train a linear classifier again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(random_state=10)\n",
    "sgd_clf.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(1,2,1)\n",
    "plot_feature_space_function(X_train, y_train, sgd_clf.decision_function)\n",
    "plt.colorbar() # colorbar shows the signed distance to the separating hyperplane.\n",
    "plt.title('hypothesis function')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plot_feature_space_function(X_train, y_train, sgd_clf.predict)\n",
    "plt.colorbar() # although the colorbar ranges from 0 to 1.05, the predicted label can only be 0 or 1 here (binary classification).\n",
    "plt.title('prediction label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating a linear classifier\n",
    "\n",
    "Now let's see if you can reproduce these figures 'manually' by reimplementing the hypothesis function of the linear classifier yourself.\n",
    "We will not focus here on training the linear classifier, so we will reuse the the trained sklearn classifier.\n",
    "\n",
    "#### Math\n",
    "Given \n",
    "* a feature vector $x = [x_1, x_2, ..., x_D]$,\n",
    "* and model parameters $[w_1, ..., w_D, b]$,\n",
    "\n",
    "Then for a linear classifier\n",
    "* the hypothesis is $h(x) = \\sum_i [ x_i . w_i ] + b$\n",
    "* the predicted label $\\widehat{y}$ is\n",
    "  * `0` if $h(x) < 0.$\n",
    "  * `1` if $h(x) >= 0.$\n",
    "\n",
    "Note that the summation in the hypothesis function can be implented efficiently using a dot product.\n",
    "\n",
    "#### Task\n",
    "Create the following functions:\n",
    "\n",
    "1. `my_lin_decision_function(sgd_clf, X)` should evaluate the hypothesis function of the linear classifier on the data in matrix X (each row is one feature vector). The outcome of this function should be the same as when calling `sgd_clf.decision_function(X)`\n",
    "\n",
    "2. `my_lin_predict(sgd_clf, X)` should return the predicted class labels for the data X, the result should be the same as when calling `sgd_clf.predict(X)`. To implement this function, first get the result of `my_lin_decision_function(sgd_clf, X)`. Assume that the negative class has label `0`, and the positive class has label `1`.\n",
    "\n",
    "\n",
    "**Tip:**\n",
    "In the first step you should extract the learned model weights from the trained sklearn classifier. Look in [the documentation of SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html?highlight=sgdclassifier#sklearn.linear_model.SGDClassifier) for the list of \"Attributes\" that the trained object will have, here you will find the name of the model weights vector $w$ and of the offset $b$ (a.k.a. \"intercept\"). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58ac2b019fb6699c238c065bc51c9283",
     "grade": false,
     "grade_id": "cell-6666366d07b97959",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def my_lin_decision_function(sgd_clf, X):\n",
    "    # Access the sgd_clf object here\n",
    "    # to get its weights and offset\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return y\n",
    "\n",
    "def my_lin_predict(sgd_clf, X):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00e2f881878568e6c3660eebff444ba3",
     "grade": true,
     "grade_id": "cell-021bb72f31f75fa3",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "my_decision = my_lin_decision_function(sgd_clf, X)\n",
    "sklearn_decision = sgd_clf.decision_function(X)\n",
    "\n",
    "assert(my_decision.shape == sklearn_decision.shape)\n",
    "assert(np.all(np.abs(my_decision - sklearn_decision) < 1e-12))\n",
    "\n",
    "my_predict = my_lin_predict(sgd_clf, X)\n",
    "sklearn_predict = sgd_clf.predict(X)\n",
    "\n",
    "assert(my_predict.shape == sklearn_predict.shape)\n",
    "assert(np.all(my_predict == sklearn_predict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have implemented the classifier correctly, the following block should generate the same feature space plots as we made before with sklearn's builtin functions. Verify that the plots are indeed the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(1,2,1)\n",
    "plot_feature_space_function(X_train, y_train, lambda X: my_lin_decision_function(sgd_clf, X))\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plot_feature_space_function(X_train, y_train, lambda X: my_lin_predict(sgd_clf, X))\n",
    "plt.colorbar() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Define a Bayesian classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h1>ðŸŽ¥ <a href=\"https://brightspace.tudelft.nl/d2l/le/content/682421/viewContent/3726354/View\">Lecture 3B - Generative models</a></h1>\n",
    "    The following requires the knowledge covered in this lecture. If you haven't watched the video yet, it's now high time to do so...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 As the first step, let's estimate the prior class probability distribution $p(y)$.\n",
    "Since this is a distribution over only two possible discrete outcomes, $y=0$ and $y=1$,\n",
    "we can characterize this distributions by an array $p = [p0, p1]$ with two element, i.e.\n",
    "\n",
    "- $p0 = p(y=0)$ is the prior probability that the a sample's class label is 0\n",
    "- $p1 = p(y=1)$ is the prior probability that the a sample's class label is 1\n",
    "\n",
    "Of course, $p0 + p1 = 1$, since these are the only possible labels.\n",
    "Implement a function that computes the array $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0e3c9e549ccb3b772062b357957a41e",
     "grade": false,
     "grade_id": "cell-ca0f63c42740ee2c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def estimate_prior(y):\n",
    "    prior = np.array([np.NaN, np.NaN])\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51a2949493546bfc7eecc03067516ff1",
     "grade": true,
     "grade_id": "cell-dfb0768d6a5369eb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "prior = estimate_prior(y)\n",
    "\n",
    "print('Prior:')\n",
    "print('    p(y=0):', prior[0])\n",
    "print('    p(y=1):', prior[1])\n",
    "\n",
    "assert(len(prior) == len(np.unique(y)))\n",
    "assert(np.all( prior >= 0. ))\n",
    "assert(np.all( prior <= 1. ))\n",
    "assert(np.sum(prior) == 1. )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 In the next step, define the likelihood term, which is the class conditional probability $p(x|y)$.\n",
    "We will use here a single 2D multivariate Gaussian distribution, i.e. $p(x|y=c) = N(x | \\mu_c, \\Sigma_c)$.\n",
    "This means that we need to estimate two parameters per class $c$:\n",
    "- the 2D mean feature vector for the class, $\\mu_c$\n",
    "- the $2 \\times 2$ covariance matrix $\\Sigma_c$\n",
    "\n",
    "These are common statistical properties that can be computed using standard numpy functions, e.g. `mean()` and `cov()`.\n",
    "\n",
    "**Note** the [numpy.cov documentation](https://numpy.org/doc/stable/reference/generated/numpy.cov.html) that the covariance is normally computed \"without bias\". This concept of bias in the covariance computation is a statistical notion, and related to the fact if we assume that we already know the mean or not. \n",
    "It is beyond the scope of this assignment to go in more detail on this difference now, but you should compute the covariance **with bias** since we *do* already compute the mean, so pass `bias=True` to the `cov()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03b947ec4be9b530e5092924772f78c5",
     "grade": false,
     "grade_id": "cell-43d4703990b355d9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def estimate_gauss2d_class_likelihood(X, y, c):\n",
    "    \"\"\" Estimate the parameters of a 2D multivatiate Gaussian for class c.\n",
    "        Input   : X, a N x D matrix of N features in D-dimensional feature space\n",
    "        Input   : y, a N-dimensional array with the class labels of data X\n",
    "        Input   :  c, the target class label, either 0 or 1, for which we want to compute the parameters\n",
    "        Returns : (mean, cov), a tuple consisting of\n",
    "                     2-dimensional mean vector of the data for class label c\n",
    "                     2x2 covariance matrix of the data for class label c\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return mean, cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72eeb961381176f7836ffb93c5f352cc",
     "grade": true,
     "grade_id": "cell-5190bb16d8e5901f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# here we estimate the Gaussian distributions for the class labels\n",
    "means = [None, None]\n",
    "covs = [None, None]\n",
    "for c in [0, 1]:\n",
    "    means[c], covs[c] = estimate_gauss2d_class_likelihood(X, y, c)\n",
    "\n",
    "# perform some checks\n",
    "assert(means[0].shape == (2,))\n",
    "assert(means[1].shape == (2,))\n",
    "assert(covs[0].shape == (2,2))\n",
    "assert(covs[1].shape == (2,2))\n",
    "# covariance matrices should be symmetric\n",
    "assert(np.all( covs[0].T == covs[0] ))\n",
    "assert(np.all( covs[1].T == covs[1] ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the estimated parameters. Below you are given a small utility function that can plot the parameters of a 2D Gaussian as ellipses that separate the high probability density region (inside the ellipse) from the low probability density region (outside the ellipse). If everything is correct, the block below should generate the same image.\n",
    "\n",
    "![reference plot of fitted parameters](extra/reference_plot_gauss_fit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gauss(mean, cov, sigmas=None, *args, **kwargs):\n",
    "    \"\"\" Plot the 2D Gaussian distribution as an ellipse.\n",
    "        The ellipse marks the points with equal probability density.\n",
    "        The width is indicate by sigma, which for a 1D gauss would be\n",
    "        the probability density at 1 std.dev. away from the mean.\n",
    "        Multiple ellipses can be drawn if multiple sigma values are given.\n",
    "    \"\"\"\n",
    "    if sigmas is None:\n",
    "        sigmas = [1.]\n",
    "    \n",
    "    S = np.linalg.cholesky(cov)\n",
    "    alphas = np.linspace(0, np.pi*2, 100)\n",
    "    xs = np.vstack((np.sin(alphas), np.cos(alphas)))\n",
    "    xs = S.dot(xs).T\n",
    "    for s in sigmas:\n",
    "        pts = xs*s + mean\n",
    "        plt.plot(pts[:,0], pts[:,1], alpha=1./s, *args, **kwargs)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "plt.plot(X[y==0,0], X[y==0,1], '.', label='class 0')\n",
    "plt.plot(X[y==1,0], X[y==1,1], '.', label='class 1')\n",
    "plot_gauss(means[0], covs[0], color='b', sigmas=[1., 2., 3.])\n",
    "# plot_gauss(means[0], covs[0], sigmas=[1., 2., 3.])\n",
    "plt.legend()\n",
    "plt.grid('on')\n",
    "plt.title('fitted 2D Gauss for class 0')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "plt.plot(X[y==0,0], X[y==0,1], '.', label='class 0')\n",
    "plt.plot(X[y==1,0], X[y==1,1], '.', label='class 1')\n",
    "plot_gauss(means[1], covs[1], color='r', sigmas=[1., 2., 3.])\n",
    "plt.legend()\n",
    "plt.grid('on')\n",
    "plt.title('fitted 2D Gauss for class 1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 Next, we must be able to evaluate these class-conditional probabilities $p(x|y=c)$ for any other point $x$ in the feature space, for each class $y=c$.\n",
    "To achieve this, we need to evaluate the probability density function of the multivariate Gaussian,\n",
    "\n",
    "$p(x|y=c) = N(x | \\mu_c, \\Sigma_c)$\n",
    "\n",
    "where the Gaussian density function is defined as\n",
    "$N(x | \\mu_c, \\Sigma_c) = \\frac{1}{\\sqrt{(2\\pi)^d |\\Sigma|}} e^{-\\frac{1}{2}(x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)}$.\n",
    "\n",
    "Luckily, this function is already implemented in the `stats` module of the scipy library,\n",
    "so you will only need to wrap this [multivariate_normal.pdf()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html#scipy-stats-multivariate-normal) function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d41a1238b0cbd371f7048e78a25f706d",
     "grade": false,
     "grade_id": "cell-96ff10a1c609b6f1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def class_likelihood(X, c, means, covs):\n",
    "    \"\"\" Compute the multivatiate Gaussian pdf on data X for class c.\n",
    "        Input:     X, a N x D matrix of N features in D-dimensional feature space\n",
    "        Input:     c, the target class label, either 0 or 1\n",
    "        Input: means, list with mean vectors of classes 0 and 1\n",
    "        Input:  covs, list with cocariance matrices of classes 0 and 1\n",
    "        Returns:   p, a N-dimensional array with the probability density of the N features for the given class c\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf0a42f91b0ca7cfb153d67dec126bee",
     "grade": true,
     "grade_id": "cell-e9e3f0aabb1125d7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# we should be able to evaluate these function for both classes\n",
    "p0 = class_likelihood(X, 0, means, covs)\n",
    "p1 = class_likelihood(X, 1, means, covs)\n",
    "\n",
    "# each call should return N probability densities\n",
    "assert(p0.shape == (X.shape[0],))\n",
    "assert(p1.shape == (X.shape[0],))\n",
    "\n",
    "# probability densities should be positive\n",
    "#   NOTE: no guarantee that they are below 1, or that they sum to 1. A probability densitiy is NOT the same thing is a probability\n",
    "assert(np.all( p0 >= 0. ))\n",
    "assert(np.all( p1 >= 0. ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented the function correctly, we should now be able to visualize the probability density for each class for each point in the space. We can reuse the function `plot_feature_space_function` to show the density using a colored overlay. The changes in the colored regions should show the same contours in the plots you made a few cells above.\n",
    "\n",
    "The cell block below should show the same plot as this:\n",
    "![reference plot of fitted parameters](extra/reference_plot_gauss_density.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_lik_c0(X):\n",
    "    return class_likelihood(X, 0, means, covs)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plot_feature_space_function(X, y, eval_lik_c0)\n",
    "plt.clim([0, 0.3]) # fix intensity scale\n",
    "plt.colorbar()\n",
    "plt.title('likelihood p(x|y=0)')\n",
    "\n",
    "def eval_lik_c1(X):\n",
    "    return class_likelihood(X, 1, means, covs)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plot_feature_space_function(X, y, eval_lik_c1)\n",
    "plt.clim([0, 0.3])\n",
    "plt.colorbar()\n",
    "plt.title('likelihood p(x|y=1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.4 Now that we can estimate the parameters of our prior and likelihood terms,\n",
    "we need to implement the functions to evaluate the posterior.\n",
    "The posterior can be found using Bayes' rule.\n",
    "In particular, we are interested in the posterior probability of the positive class (i.e. $y = 1$),\n",
    "which with Bayes' rule is computed as\n",
    "\n",
    "$ p(y=1|x) = \\frac{p(x|y=1) p(y=1)}{p(x|y=0) p(y=0) + p(x|y=1) p(y=1)}$\n",
    "\n",
    "Implement the function `class_posterior()`, which should compute this posterior probability $p(y=1|x)$ for all the feature vectors in X.\n",
    "The given function `bayes_classifier_predict()` should accordingly return the class label `1` when the posterior for $y=1$ is more probable, i.e. at least 50%. Otherwise it will return the label `0`.\n",
    "\n",
    "Reuse the function `class_likelihood()` that you wrote before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd3c17b5077a6a375aa1846eb715dd39",
     "grade": false,
     "grade_id": "cell-82c2f2c7d3057052",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def class_posterior(X, prior, means, covs):\n",
    "    \"\"\" Compute posterior probability for class y = 1 for all feature vectors in X\n",
    "        Input  : X, a N x D matrix of N features in D-dimensional feature space\n",
    "        Input  : priors, an array with the prior proabilities of class 0 and 1\n",
    "        Input  : means, list with mean vectors of classes 0 and 1\n",
    "        Input  : covs, list with cocariance matrices of classes 0 and 1\n",
    "        Returns: post, a N-dimensional vector containing the posterior probabilities of y=1 for the given features.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return post\n",
    "\n",
    "def bayes_classifier_predict(X, prior, means, covs):\n",
    "    return class_posterior(X, prior, means, covs) >= 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff508680adf80ae0d6be95da519357bc",
     "grade": true,
     "grade_id": "cell-015d6bf78165a608",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "p = class_posterior(X, prior, means, covs)\n",
    "\n",
    "# this should contain the probability for class y = 1\n",
    "assert(p.shape == (X.shape[0],))\n",
    "assert(np.all( p >= 0. )) \n",
    "assert(np.all( p <= 1. )) # this should hold, since probability, not probability density\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last check, let's plot all the steps together to highlight how the decision boundary is constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_lik_c0(X):\n",
    "    return class_likelihood(X, 0, means, covs)\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.subplot(1,4,1)\n",
    "plot_feature_space_function(X, y, eval_lik_c0)\n",
    "plt.clim([0, 0.3]) # fix intensity scale\n",
    "#plt.colorbar()\n",
    "plt.title('likelihood p(x|y=0)')\n",
    "\n",
    "def eval_lik_c1(X):\n",
    "    return class_likelihood(X, 1, means, covs)\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "plot_feature_space_function(X, y, eval_lik_c1)\n",
    "plt.clim([0, 0.3])\n",
    "#plt.colorbar()\n",
    "plt.title('likelihood p(x|y=1)')\n",
    "\n",
    "def eval_posterior(X):\n",
    "    return class_posterior(X, prior, means, covs)\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plot_feature_space_function(X, y, eval_posterior)\n",
    "plt.clim([0, 1.0]) # fix intensity scale\n",
    "#plt.colorbar()\n",
    "plt.title('posterior p(y=1|x)');\n",
    "\n",
    "def eval_predict(X):\n",
    "    return bayes_classifier_predict(X, prior, means, covs)\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "plot_feature_space_function(X, y, eval_predict)\n",
    "plt.clim([0, 1.0]) # fix intensity scale\n",
    "#plt.colorbar()\n",
    "plt.title('decision boundary');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Creating a new sklearn Classifier\n",
    "\n",
    "Now that we have performed all these steps in isolation, let's join them into a custom sklearn classifier class.\n",
    "This is done by subclassing sklearn's `BaseEstimator` and `ClassifierMixin`,\n",
    "as is explained in more detail [in the sklearn documentation on rolling your own classifier](https://scikit-learn.org/stable/developers/develop.html#rolling-your-own-estimator).\n",
    "We have already given you the main skeleton in the cell below, though.\n",
    "\n",
    "A few notes:\n",
    "\n",
    "- sklearn provides some utility functions for common checks, such as `check_X_y(X,y)` to test if training data X and y have the right shape and format (otherwise an exception will be thrown, alerting the user). These utility functions have already been added for your convenience.\n",
    "\n",
    "\n",
    "- Do not do any data or parameter validation in the initializer, only do this is in the fit() member function. See [parameters and init in the sklearn documentation](https://scikit-learn.org/stable/developers/develop.html#parameters-and-init) for more info on why. Since our Bayesian classifier doesn't have any hyperparameters at all, the initialization function can remain empty (we still need a `pass` statement which does nothing to make it legal python code though with proper identation)\n",
    "\n",
    "- Like we have seen with the SGDClassifier before, sklearn has the convention that trained model parameters terminate with a single underscore '\\_'. So call your members `self.prior_` instead of `self.prior`, for instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0c22346a76cd9b9bef0a3c0ef78f978",
     "grade": false,
     "grade_id": "cell-9d77496d82941415",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "class MyBayesClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        # do NOT do anything in the initializer\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # Check is fit had been called\n",
    "        #   If the fit() function was not called yet, this check_is_fitted(self) will throw an exception.\n",
    "        #   It works by checking if the object contains a attributes with a trailing '_', i.e. learned model\n",
    "        #   parameters, which should have only been set in your fit() function.\n",
    "        # NOTE: we have disabled this check here because check_is_fitted() does not behave as \n",
    "        #   expected in some older versions of sklearn, see https://stackoverflow.com/questions/60432260/typeerror-check-is-fitted-missing-1-required-positional-argument-attributes\n",
    "        #   We'll keep it here as a reminder for future iterations of this course!\n",
    "        #   So, you don't need to uncomment this, but if you run the predict step without fitting, \n",
    "        #   your code will still throw an exception as you will try to access attributes that have not been set yet.\n",
    "        #check_is_fitted(self)\n",
    "        \n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0abaf35c35db849748d08946ab6364e9",
     "grade": true,
     "grade_id": "cell-f04ac191d7851276",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "bayes_gauss_clf = MyBayesClassifier()\n",
    "\n",
    "# this should work\n",
    "bayes_gauss_clf.fit(X, y)\n",
    "y_pred = bayes_gauss_clf.predict(X)\n",
    "\n",
    "\n",
    "# when X and y do not have the right format, a ValueError should be thrown.\n",
    "#    (this should be done automatically if you left the check_X_y() call in place)\n",
    "def should_throw_exception(f):\n",
    "    \"\"\" test if function f throws the expected exception \"\"\"\n",
    "    try:\n",
    "        f()\n",
    "    except:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# wrong input should throw an exception    \n",
    "assert( should_throw_exception(lambda : bayes_gauss_clf.fit(X.T, y)) )\n",
    "\n",
    "# predicting with a non-fitted object should throw an exception\n",
    "clf_not_fitted = MyBayesClassifier()\n",
    "assert( should_throw_exception(lambda : clf_not_fitted.predict(X)) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your Bayesian classifer should be compatible with most sklearn utilities,\n",
    "and we can use the same `.fit()` and `.predict()` interface as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_gauss_clf = MyBayesClassifier()\n",
    "bayes_gauss_clf.fit(X, y)\n",
    "\n",
    "plot_feature_space_function(X, y, bayes_gauss_clf.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Create a Gaussian-Mixture Bayesian classifier\n",
    "\n",
    "The Bayesian classifier you just wrote used a single multi-variate Gaussian to represent each class's data distribution. However, this is a pretty strong assumption on the shape of the class data.\n",
    "We can make a more flexible classifier by describing each class's data instead with a different more complex distribution. If we can fit this complex distribution, and evaluate its probability density for new feature vectors, we can still plug it in the Bayesian formulation and compute a class posterior distribution as before.\n",
    "\n",
    "\n",
    "A general method to approximate non-Gaussian distributions is to use not 1 but multiple Gaussians, each with their own mean and covariance. The full distribution is then a weighted combinations of these separate Gaussian components. Such a distribution is called a **Gaussian Mixture**.\n",
    "\n",
    "This figure shows *left* a classifier using a Gaussian per class (like you wrote before),\n",
    "and *right* a classifier using per-class a Gaussian Mixture with 2 components.\n",
    "![Gaussian vs Gaussian-Mixture classier](extra/gaussian_vs_mixture_classifier.png)\n",
    "\n",
    "### fitting a Gaussian Mixture distribution\n",
    "Unlike with a single Gaussian, where model parameters can be directly computed in closed form, fitting a Gaussian Mixture requires an iterative optimization scheme. This scheme iterates between two steps:\n",
    "1. figuring out which data points are covered by which Gaussian component\n",
    "2. fit the Gaussian components on the data points that they cover.\n",
    "\n",
    "After the parameters of the Gaussian Mixture have been altered in step 2, the Gaussian components may cover the data slightly differently, hence the next iteration must start again with step 1.\n",
    "\n",
    "### using sklearn's Gaussian Mixture\n",
    "\n",
    "Luckily, the `sklearn.mixture` module contains a ready made `GaussianMixture` class that can be used to fit a Gaussian Mixture distribution on given data:\n",
    "\n",
    "- initialize a Gaussian mixture with $n$ mixture components using `gmm = GaussianMixture(n_components=n)` \n",
    "- fit the GMM distribution on data `X` using the iterative method through the object `gmm.fit(X)` function (should be pretty quick for low-dimensional data). Note that you *don't* pass any class labels `y` since the GaussianMixture is not a classifier or regressor, but just an (unsupervised) density estimator.\n",
    "- you can evaluate the **log** probability density for features `X` using the `gmm.score_samples(X)` member function. Note that because this is the log density, the returned values can be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand how the Gaussian Mixture works, let's play around with it and visualize the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some more dummy data to demonstrate a Gaussian Mixture\n",
    "from sklearn.datasets import make_blobs\n",
    "X_, y_ = make_blobs(n_samples=200, centers=4, n_features=2, random_state=0)\n",
    "y_[:] = 0 # ignore the class labels in our visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_gmm(n_components=3):\n",
    "    gmm = GaussianMixture(n_components=n_components)\n",
    "    gmm.fit(X_)\n",
    "\n",
    "    plot_feature_space_function(X_, y_, gmm.score_samples)\n",
    "    plt.colorbar()\n",
    "    \n",
    "ipywidgets.interact(demo_gmm, n_components=(1,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the individual component in the Gaussian mixture to better illustrate how the overall distribution's density is composed of these components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gmm(gmm, sigmas=None, *args, **kwargs):\n",
    "    K = gmm.means_.shape[0]\n",
    "    for k in range(K):\n",
    "        mean = gmm.means_[k]\n",
    "        cov = gmm.covariances_[k]\n",
    "        plot_gauss(mean, cov, sigmas=sigmas, *args, **kwargs)\n",
    "\n",
    "def demo_gmm2(n_components=3):\n",
    "    gmm = GaussianMixture(n_components=n_components)\n",
    "    gmm.fit(X_)\n",
    "\n",
    "    plot_feature_space_function(X_, y_, gmm.score_samples)\n",
    "    plot_gmm(gmm)\n",
    "    plt.colorbar()\n",
    "    \n",
    "ipywidgets.interact(demo_gmm2, n_components=(1,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.1 Now implement a Gaussian Mixture Model (GMM) classifier in the cell below.\n",
    "You can look at your `MyBayesClassifier` class above, and follow the same steps.\n",
    "\n",
    "Some hints:\n",
    "- you can reuse the `estimate_prior()` function, since the formulation of the prior is unaffected by the change\n",
    "- you should NOT use your `estimate_gauss2d_class_likelihood()` and `class_likelihood()` functions anymore, since these was specificly made for a single 2D Gaussian. Instead store for each class a `GaussianMixture()` object in your fit step, and use its `score_samples()` function to compute the likelihood term in the predict step.\n",
    "- Note that `GaussianMixture.score_samples` return a *log* probability density, you should apply `np.exp()` to get a regular probability density for your posterior calculation.\n",
    "- because you can't reuse your `class_likelihood()` function, you also can't reuse your previous `bayes_classifier_predict()` function.\n",
    "- you can use the `.fit()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ced86bda24a8c38503060e6ed93b7ab5",
     "grade": false,
     "grade_id": "cell-5fd8f360b257a283",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "class MyGmmClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_components=1):\n",
    "        # define hyperparameters here\n",
    "        self.n_components = n_components\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # Check is fit had been called\n",
    "        #   If the fit() function was not called yet, this check_is_fitted(self) will throw an exception.\n",
    "        #   It works by checking if the object contains a attributes with a trailing '_', i.e. learned model\n",
    "        #   parameters, which should have only been set in your fit() function.\n",
    "        #\n",
    "        # NOTE: we have disabled this check here because check_is_fitted() does not behave as \n",
    "        #   expected in some older versions of sklearn, see https://stackoverflow.com/questions/60432260/typeerror-check-is-fitted-missing-1-required-positional-argument-attributes\n",
    "        #   We'll keep it here as a reminder for future iterations of this course!\n",
    "        #   So, you don't need to uncomment this, but if you run the predict step without fitting, \n",
    "        #   your code will still throw an exception as you will try to access attributes that have not been set yet.\n",
    "        #check_is_fitted(self)\n",
    "        \n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf945a4326fc556bebc0413ce3eb8236",
     "grade": true,
     "grade_id": "cell-0dc7d90df4fa2d9f",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "gmm_clf = MyGmmClassifier(n_components=2)\n",
    "\n",
    "# this should work\n",
    "gmm_clf.fit(X, y)\n",
    "y_pred = gmm_clf.predict(X)\n",
    "\n",
    "\n",
    "# when X and y do not have the right format, a ValueError should be thrown.\n",
    "#    (this should be done automatically if you left the check_X_y() call in place)\n",
    "def should_throw_exception(f):\n",
    "    \"\"\" test if function f throws the expected exception \"\"\"\n",
    "    try:\n",
    "        f()\n",
    "    except:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# wrong input should throw an exception    \n",
    "assert( should_throw_exception(lambda : gmm_clf.fit(X.T, y)) )\n",
    "\n",
    "# predicting with a non-fitted object should throw an exception\n",
    "# because your predict() step should be accessing the not yet set attributes with model parameters\n",
    "clf_not_fitted = MyGmmClassifier(n_components=2)\n",
    "assert( should_throw_exception(lambda : clf_not_fitted.predict(X)) )\n",
    "\n",
    "# The GMM with 1 component should give the same\n",
    "#   predictions as the original single Gaussian Bayesian classifier\n",
    "gmm_clf = MyGmmClassifier(n_components=1)\n",
    "gmm_clf.fit(X, y)\n",
    "gmm_y_pred = gmm_clf.predict(X)\n",
    "bayes_gauss_clf = MyBayesClassifier()\n",
    "bayes_gauss_clf.fit(X, y)\n",
    "gauss_y_pred = bayes_gauss_clf.predict(X)\n",
    "assert(np.all( gmm_y_pred == gauss_y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.2 Let's visualize the classification boundary of the Gaussian-Mixture Bayesian classifier\n",
    "when using 1 or 3 components per class.\n",
    "\n",
    "For reference, we also show again the boundary of the first (non-mixture) Bayesian classifier you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_gauss_clf = MyBayesClassifier()\n",
    "bayes_gauss_clf.fit(X, y)\n",
    "\n",
    "gmm1_clf = MyGmmClassifier(n_components=1)\n",
    "gmm1_clf.fit(X, y)\n",
    "\n",
    "gmm3_clf = MyGmmClassifier(n_components=3)\n",
    "gmm3_clf.fit(X, y)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.subplot(1,3,1)\n",
    "plot_feature_space_function(X, y, bayes_gauss_clf.predict)\n",
    "plt.title('Original Bayesian classifier')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plot_feature_space_function(X, y, gmm1_clf.predict)\n",
    "plt.title('GMM, 1 component per class')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plot_feature_space_function(X, y, gmm3_clf.predict)\n",
    "plt.title('GMM, 3 components per class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is correct, the GMM with 1 component per class should show the same boundary as your first classifier (Try to answer for yourself: why should these be the same?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Visually understanding model complexity, overfitting, Bias & Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h1>ðŸŽ¥ <a href=\"https://brightspace.tudelft.nl/d2l/le/content/682421/viewContent/3726353/View\">Watch Lecture 3C - Model complexity</a></h1>\n",
    "    The following requires the knowledge covered in this lecture. If you haven't watched the video yet, it's now high time to do so...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Take a dataset, alter it a little bit to make it more interesting ;)\n",
    "ds = datasets.load_wine()\n",
    "X = ds['data']\n",
    "X = X[:,:2]\n",
    "y = ds['target']\n",
    "y = (y == 2).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_train_test_feature_space(clf, res=0.05):\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    \n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plot_feature_space_function(X_train, y_train, clf.predict, res=res)\n",
    "    plt.title(f'Train split, accuracy {train_acc:.3f}, f1 {train_f1:.3f}')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plot_feature_space_function(X_test, y_test, clf.predict, res=res)\n",
    "    plt.title(f'Test split, accuracy = {test_acc:.3f}, f1 {test_f1:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def show_knn_train_test(k=1):\n",
    "    print(k)\n",
    "    knn_clf = KNeighborsClassifier(k)\n",
    "    knn_clf.fit(X_train, y_train)\n",
    "    show_train_test_feature_space(knn_clf, res=0.05)\n",
    "\n",
    "ipywidgets.interact(show_knn_train_test, k=ipywidgets.IntSlider(value=1,min=1, max=25,step=2, continuous_update=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.1 Using the interactive widget above, explore how the hyperparameter $k$ affects the decision boundary of the k-Nearest Neighbor classifier.\n",
    "\n",
    "**Q** When does the KNN generate the more complex decision boundary? At low or high k values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e406f8cb6280e5273c73001101052b6",
     "grade": false,
     "grade_id": "cell-2b55bce1b58c05b1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer by uncommenting only the correct options from this block below\n",
    "\n",
    "KNN_COMPLEX_BOUNDARY_WHEN_K_IS = '?'\n",
    "#KNN_COMPLEX_BOUNDARY_WHEN_K_IS = 'small'\n",
    "#KNN_COMPLEX_BOUNDARY_WHEN_K_IS = 'large'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "989f1ce9a5986b9d5de22598c51f1cb0",
     "grade": true,
     "grade_id": "cell-3341dc0975a48c90",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('Your answer:', KNN_COMPLEX_BOUNDARY_WHEN_K_IS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.2 Now that we understand how the hyperparameters of the KNN can generate simple versus complex decision boundaries, let's explore how sensitive these models are to variations in the data.\n",
    "To do this, we will see what happens to the decision if for different random subsets of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_knn_variance(k):\n",
    "    plt.figure(figsize=(16,4))\n",
    "\n",
    "    # train the k-NN classifier with different random splits of the training data\n",
    "    for j, random_state in enumerate([1,10,42,100]):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=random_state)\n",
    "        \n",
    "        knn_clf = KNeighborsClassifier(k)\n",
    "        knn_clf.fit(X_train, y_train)\n",
    "\n",
    "        plt.subplot(1,4,j+1)\n",
    "        plot_feature_space_function(X_train, y_train, knn_clf.predict)\n",
    "        plt.title(f'random seed {j+1}')\n",
    "\n",
    "ipywidgets.interact(show_knn_variance, k=ipywidgets.IntSlider(value=1,min=1, max=13,step=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect using the interactive plot above how the $k$ affect the decision boundaries,\n",
    "and how sensitive the classifier becomes for different subsets of the data.\n",
    "\n",
    "Using the visualizations, answer the following questions:\n",
    "\n",
    "- When does the KNN show high bias?\n",
    "\n",
    "- When does the KNN show high variance?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fbe55f1cbdcc9efb9f57e146f34b2bd1",
     "grade": false,
     "grade_id": "cell-17d3d791f2fbb8c0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "KNN_MOST_BIAS_WHEN_K_IS = '?'\n",
    "#KNN_MOST_BIAS_WHEN_K_IS = 'small'\n",
    "#KNN_MOST_BIAS_WHEN_K_IS = 'large'\n",
    "\n",
    "KNN_MOST_VARIANCE_WHEN_K_IS = '?'\n",
    "#KNN_MOST_VARIANCE_WHEN_K_IS = 'small'\n",
    "#KNN_MOST_VARIANCE_WHEN_K_IS = 'large'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b8bcc5822b15ae2b11db23d093dc45a",
     "grade": true,
     "grade_id": "cell-1ad18cae0b1308f9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('Q: When does the KNN show most bias, when k is small or large?')\n",
    "print('A:', KNN_MOST_BIAS_WHEN_K_IS)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74012e803cae8dc91a9d1081e3f6a653",
     "grade": true,
     "grade_id": "cell-c7e3ea1ad3f85a99",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('Q: When does the KNN show most variance, when k is small or large?')\n",
    "print('A:', KNN_MOST_VARIANCE_WHEN_K_IS)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.3 using your custom Gaussian-Mixture Model\n",
    "\n",
    "Now let's try to do the same thing with the Gaussian-Mixture Bayesian classifier you built earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gmm_train_test(n_components=1):\n",
    "    print(n_components)\n",
    "    gmm_clf = MyGmmClassifier(n_components=n_components)\n",
    "    gmm_clf.fit(X_train, y_train)\n",
    "    show_train_test_feature_space(gmm_clf, res=0.01)\n",
    "\n",
    "ipywidgets.interact(show_gmm_train_test, n_components=ipywidgets.IntSlider(value=1,min=1, max=15, continuous_update=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** When does the GMM generate the more complex decision boundary? At low or high number of components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ac299b896898b2ed5ba0f4b01168617",
     "grade": false,
     "grade_id": "cell-733bcf4daec4b3c0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer by uncommenting only the correct options from this block below\n",
    "\n",
    "GMM_COMPLEX_BOUNDARY_WHEN_NUMCOMP_IS = '?'\n",
    "#GMM_COMPLEX_BOUNDARY_WHEN_NUMCOMP_IS = 'small'\n",
    "#GMM_COMPLEX_BOUNDARY_WHEN_NUMCOMP_IS = 'large'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76a7a25343480f4b7153fa4f7c1db367",
     "grade": true,
     "grade_id": "cell-a6e2a90bcd59fcbb",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('Your answer:', GMM_COMPLEX_BOUNDARY_WHEN_NUMCOMP_IS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gmm_variance(n_components):\n",
    "    plt.figure(figsize=(16,4))\n",
    "\n",
    "    # train the GMM classifier with different random splits of the training data\n",
    "    for j, random_state in enumerate([1,10,42,100]):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=random_state)\n",
    "        gmm_clf = MyGmmClassifier(n_components=n_components)\n",
    "        gmm_clf.fit(X_train, y_train)\n",
    "\n",
    "        plt.subplot(1,4,j+1)\n",
    "        plot_feature_space_function(X_train, y_train, gmm_clf.predict, res=0.02)\n",
    "        plt.title(f'random seed {j+1}')\n",
    "\n",
    "ipywidgets.interact(show_gmm_variance, n_components=(1,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.4 Inspect using the interactive plot above how the number of components affect the decision boundaries,\n",
    "and how sensitive the classifier becomes for different subsets of the data.\n",
    "\n",
    "Using the visualizations, answer the following questions:\n",
    "* When does the GMM show high bias?\n",
    "* When does the GMM show high variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abca0515586f7ee2673840d95c353316",
     "grade": false,
     "grade_id": "cell-3a80abc37ca30183",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "GMM_MOST_BIAS_WHEN_NUMCOMP_IS = '?'\n",
    "#GMM_MOST_BIAS_WHEN_NUMCOMP_IS = 'small'\n",
    "#GMM_MOST_BIAS_WHEN_NUMCOMP_IS = 'large'\n",
    "\n",
    "GMM_MOST_VARIANCE_WHEN_NUMCOMP_IS = '?'\n",
    "#GMM_MOST_VARIANCE_WHEN_NUMCOMP_IS = 'small'\n",
    "#GMM_MOST_VARIANCE_WHEN_NUMCOMP_IS = 'large'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "056e77fa16d773327b1ba18ec9bf9a39",
     "grade": true,
     "grade_id": "cell-163a11f45af13f74",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('Q: When does the GMM show most bias, when # components is small or large?')\n",
    "print('A:', GMM_MOST_BIAS_WHEN_NUMCOMP_IS)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "921b54adf5bbcd200d41dce61a723fa5",
     "grade": true,
     "grade_id": "cell-a69023c195052f82",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('Q: When does the GMM show most variance, when # components is small or large?')\n",
    "print('A:', GMM_MOST_VARIANCE_WHEN_NUMCOMP_IS)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Regularization\n",
    "\n",
    "In this part, we will explore the effect of using regularization, and how it affects the decision boundary.\n",
    "\n",
    "The Support Vector Machine (SVM) is classifier with a regalurization parameter $C$.\n",
    "The SVM can be used to estimate a linear classification boundary (`SVC(kernel=linear)`),\n",
    "or a potentially more complex decision boundary with the so-called Radial Basis Function kernel (`SVC(kernel=rbf)`, the default kernel of sklearn's SVC). Note the sklearn calls this a Support Vector Classifier, SVC, instead of SVM.\n",
    "\n",
    "Future lectures will discuss SVMs and kernel methods in more detail, so for now we will treat the SVM with RBF kernel as a black-box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = sklearn.svm.SVC(kernel='rbf')\n",
    "\n",
    "def show_svm_train_test(C):\n",
    "    svm_clf.set_params(C=C)\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    show_train_test_feature_space(svm_clf, res=0.05)\n",
    "    \n",
    "    l2_weight = svm_clf.dual_coef_.dot(svm_clf.dual_coef_.T).flatten()[0]\n",
    "\n",
    "    print('C =', C)\n",
    "    print('norm of coefficients:', l2_weight)\n",
    "\n",
    "ipywidgets.interact(show_svm_train_test, C=ipywidgets.FloatLogSlider(1.0, base=10, min=-1, max=8, continuous_update=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_svm_variance(C):\n",
    "    plt.figure(figsize=(16,4))\n",
    "    for j, random_state in enumerate([1,10,42,100]):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=random_state)\n",
    "        svm_clf.set_params(C=C)\n",
    "        svm_clf.fit(X_train, y_train)\n",
    "\n",
    "        plt.subplot(1,4,j+1)\n",
    "        plot_feature_space_function(X_train, y_train, svm_clf.predict, res=0.02)\n",
    "        plt.title(f'random seed {j+1}')\n",
    "    print(C)\n",
    "\n",
    "ipywidgets.interact(show_svm_variance, C=ipywidgets.FloatLogSlider(1.0, base=10, min=0, max=10, continuous_update=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When is C most strongly regularizing the SVM, when it is large or small?\n",
    "* When does the SVM show most bias, when C is small or large?\n",
    "* When does the SVM show most variance, when C is small or large?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e25fb4bf09e254d263596b9226ee3fbc",
     "grade": false,
     "grade_id": "cell-581729f6acdb907d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer by uncommenting only the correct options from this block below\n",
    "\n",
    "SVM_STRONGEST_REGULARIZATION_WHEN_C_IS = '?'\n",
    "#SVM_STRONGEST_REGULARIZATION_WHEN_C_IS = 'small'\n",
    "#SVM_STRONGEST_REGULARIZATION_WHEN_C_IS = 'large'\n",
    "\n",
    "SVM_MOST_BIAS_WHEN_C_IS = '?'\n",
    "#SVM_MOST_BIAS_WHEN_C_IS = 'small'\n",
    "#SVM_MOST_BIAS_WHEN_C_IS = 'large'\n",
    "\n",
    "SVM_MOST_VARIANCE_WHEN_C_IS = '?'\n",
    "#SVM_MOST_VARIANCE_WHEN_C_IS = 'small'\n",
    "#SVM_MOST_VARIANCE_WHEN_C_IS = 'large'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e282214ef0b773f3f39bb08fd98d869",
     "grade": true,
     "grade_id": "cell-9249e90f77351317",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('Q: When is C most strongly regularizing the SVM, when it is large or small?')\n",
    "print('A:', SVM_STRONGEST_REGULARIZATION_WHEN_C_IS)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2ab85cb653a231de7d413927d25d6eb",
     "grade": true,
     "grade_id": "cell-f778f297507d687a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('Q: When does the SVM show most bias, when C is small or large?')\n",
    "print('A:', SVM_MOST_BIAS_WHEN_C_IS)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81f4370e0aad7159e65deb15571ce003",
     "grade": true,
     "grade_id": "cell-1949818692cd44e6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('Q: When does the SVM show most variance, when C is small or large?')\n",
    "print('A:', SVM_MOST_VARIANCE_WHEN_C_IS)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d50ea25cd91c190c0fd09aa3327a0e23",
     "grade": false,
     "grade_id": "cell-d4b5d6611c4c3b54",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "That wraps up this part of the exercises!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
